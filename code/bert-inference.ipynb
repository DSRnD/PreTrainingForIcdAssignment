{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"adfebac5-b6d7-4e18-93a5-a2f9a7ad93aa","_cell_guid":"8046ea75-1244-44a9-8490-b058596e9a4a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-26T09:47:45.007621Z","iopub.execute_input":"2022-04-26T09:47:45.007897Z","iopub.status.idle":"2022-04-26T09:47:45.092361Z","shell.execute_reply.started":"2022-04-26T09:47:45.007859Z","shell.execute_reply":"2022-04-26T09:47:45.091094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = 10000\nMAX_SEQUENCE_LENGTH = 16\nWORD_EMBEDDING_DIM = 200\nbatch_size = 640\nENCODING_DIM = 768\nNUM_DATAPOINT = -1 # All in the file\nNUM_ICD_CODES = 2833\nNUM_EPOCHS = 2\nVALIDATION_SPLIT = 0.2\nPATIENCE = 5\nTOP_K_PER_SENTENCE = 10\nsimilarity_threshold = 0.7\n\nTHRESHOLD = -1 # TODO\n\nICD_DESC_FILENAME = '/kaggle/input/mimicdata/ICD_desc_with_freq.csv'\n#DD_FILENAME = 'MIMIC_III_Final_1000_clean_shorted.csv'\n#DD_FILENAME = 'MIMIC-III-Final_5000_clean_shortened.csv'\nDD_FILENAME = '/kaggle/input/mimic-clean-text/MIMIC-III-Final_cleaned.csv'\n#EMB_FILE = '/kaggle/input/pmcmodel/PMC-w2v.bin'\n\n#RESULT_FILENAME = 'Experiment Results.xlsx'","metadata":{"_uuid":"78b8db53-5691-4597-8a96-c063e8ac4d1e","_cell_guid":"8af462fd-8ec5-481a-9c0d-10723abb8b2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-26T09:47:45.094709Z","iopub.execute_input":"2022-04-26T09:47:45.094963Z","iopub.status.idle":"2022-04-26T09:47:45.100223Z","shell.execute_reply.started":"2022-04-26T09:47:45.094928Z","shell.execute_reply":"2022-04-26T09:47:45.099413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:45.102007Z","iopub.execute_input":"2022-04-26T09:47:45.102351Z","iopub.status.idle":"2022-04-26T09:47:45.111183Z","shell.execute_reply.started":"2022-04-26T09:47:45.102312Z","shell.execute_reply":"2022-04-26T09:47:45.110403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport six\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:45.112585Z","iopub.execute_input":"2022-04-26T09:47:45.112822Z","iopub.status.idle":"2022-04-26T09:47:45.123021Z","shell.execute_reply.started":"2022-04-26T09:47:45.112785Z","shell.execute_reply":"2022-04-26T09:47:45.122169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleanText(sentence):\n        sentence = re.sub(\n        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n        sentence = re.sub(r\"\\,+\", \",\", sentence)\n        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n        sentence = re.sub(r'[0-9]', \" \", sentence)\n        sentence = re.sub(r'[^\\w\\s]','',sentence)\n        sentence = re.sub(r'(?:^| )\\w(?:$| )', ' ', (sentence)).strip()#AKS\n        sentence = sentence.lower()\n        #return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]\n        return sentence","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:45.124288Z","iopub.execute_input":"2022-04-26T09:47:45.124576Z","iopub.status.idle":"2022-04-26T09:47:45.279803Z","shell.execute_reply.started":"2022-04-26T09:47:45.124535Z","shell.execute_reply":"2022-04-26T09:47:45.279003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def processTextIntoSentences(text):\n    if(not len(text)):\n        print(\"\\nGiven string is empty. Please check again\")\n    sentenceSize  = 125\n    #sentenceSize = 256\n    line = convert_to_unicode(text)\n    line  = cleanText(text)\n    tokens = tokenizer.tokenize(line)\n    #print(\"\\nTokens\", tokens)\n    if(tokens):\n        all_tokens = [tokens[i: i+sentenceSize] for i in range(0, len(tokens), sentenceSize)]\n        #print(\"\\nbatched tokens\", len(all_tokens), all_tokens)\n        processedTokens = []\n        for token_ in all_tokens:\n            processedTokens.append(\" \".join(token_))\n        #print(len(processedTokens[1].split()))\n        return (\"|\".join(processedTokens))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:45.281278Z","iopub.execute_input":"2022-04-26T09:47:45.281686Z","iopub.status.idle":"2022-04-26T09:47:45.2899Z","shell.execute_reply.started":"2022-04-26T09:47:45.281643Z","shell.execute_reply":"2022-04-26T09:47:45.289104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport tqdm\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:45.402584Z","iopub.execute_input":"2022-04-26T09:47:45.403217Z","iopub.status.idle":"2022-04-26T09:47:45.408569Z","shell.execute_reply.started":"2022-04-26T09:47:45.403179Z","shell.execute_reply":"2022-04-26T09:47:45.407483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:46.554926Z","iopub.execute_input":"2022-04-26T09:47:46.555201Z","iopub.status.idle":"2022-04-26T09:47:46.559808Z","shell.execute_reply.started":"2022-04-26T09:47:46.555171Z","shell.execute_reply":"2022-04-26T09:47:46.559069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data =  pd.read_csv('/kaggle/input/mimic-clean-text/MIMIC-III-Final_cleaned.csv')[:15000]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:47:47.453178Z","iopub.execute_input":"2022-04-26T09:47:47.454098Z","iopub.status.idle":"2022-04-26T09:48:06.304928Z","shell.execute_reply.started":"2022-04-26T09:47:47.454056Z","shell.execute_reply":"2022-04-26T09:48:06.304127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BasicTokenizer(object):\n  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n  def __init__(self, do_lower_case=True):\n    \"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    \"\"\"Tokenizes a piece of text.\"\"\"\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)#perform invalid char removal and whitespace removal\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    \"\"\"Strips accents from a piece of text.\"\"\"\n    text = unicodedata.normalize(\"NFD\", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == \"Mn\":\n        continue\n      output.append(char)\n    return \"\".join(output)\n\n  def _run_split_on_punc(self, text):\n    \"\"\"Splits punctuation on a piece of text.\"\"\"\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return [\"\".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    \"\"\"Adds whitespace around any CJK character.\"\"\"\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append(\" \")\n        output.append(char)\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)\n\n  def _is_chinese_char(self, cp):\n    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.306761Z","iopub.execute_input":"2022-04-26T09:48:06.307008Z","iopub.status.idle":"2022-04-26T09:48:06.328285Z","shell.execute_reply.started":"2022-04-26T09:48:06.306973Z","shell.execute_reply":"2022-04-26T09:48:06.327448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_vocab(vocab_file):\n  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n  vocab = collections.OrderedDict()\n  index = 0\n  #with tf.gfile.GFile(vocab_file, \"r\") as reader:\n  with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = (token.strip()).split(\"\\t\")\n      vocab[token[0]] = index\n      index += 1\n  print(\"\\nVocab loading done!\")\n  return vocab","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.331085Z","iopub.execute_input":"2022-04-26T09:48:06.331782Z","iopub.status.idle":"2022-04-26T09:48:06.339136Z","shell.execute_reply.started":"2022-04-26T09:48:06.331717Z","shell.execute_reply":"2022-04-26T09:48:06.33826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WordpieceTokenizer(object):\n  \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    \"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        #print(\"\\nunknown token inserted\")\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = \"\".join(chars[start:end])\n          if start > 0:\n            substr = \"##\" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            #print(\"\\nknown token inserted\", cur_substr)\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n        #print(\"\\nunknown token inserted\") #This is where unknown tokens are being inserted\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return True\n  cat = unicodedata.category(char)\n  if cat == \"Zs\":\n    return True\n  return False\n\n\ndef _is_control(char):\n  \"\"\"Checks whether `chars` is a control character.\"\"\"\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (\"Cc\", \"Cf\"):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(\"P\"):\n    return True\n  return False","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.341422Z","iopub.execute_input":"2022-04-26T09:48:06.341889Z","iopub.status.idle":"2022-04-26T09:48:06.358664Z","shell.execute_reply.started":"2022-04-26T09:48:06.341848Z","shell.execute_reply":"2022-04-26T09:48:06.357913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FullTokenizer(object):\n  \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)#Create Vocab dist with index 0\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}#Get vocab dict with index as key and token as value\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)#Initialize the basic tokenizer \n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      #for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.360086Z","iopub.execute_input":"2022-04-26T09:48:06.360494Z","iopub.status.idle":"2022-04-26T09:48:06.371251Z","shell.execute_reply.started":"2022-04-26T09:48:06.360455Z","shell.execute_reply":"2022-04-26T09:48:06.370554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_by_vocab(vocab, items):\n  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.372606Z","iopub.execute_input":"2022-04-26T09:48:06.373083Z","iopub.status.idle":"2022-04-26T09:48:06.383558Z","shell.execute_reply.started":"2022-04-26T09:48:06.373044Z","shell.execute_reply":"2022-04-26T09:48:06.382646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = FullTokenizer(\n      vocab_file='/kaggle/input/autoencoderdecoderonpmcembeddingvocab/Vocab.txt', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.385958Z","iopub.execute_input":"2022-04-26T09:48:06.386418Z","iopub.status.idle":"2022-04-26T09:48:06.556407Z","shell.execute_reply.started":"2022-04-26T09:48:06.386376Z","shell.execute_reply":"2022-04-26T09:48:06.555671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['processedText'] = data.apply(lambda x:processTextIntoSentences(x['clean_text']) , axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:48:06.557521Z","iopub.execute_input":"2022-04-26T09:48:06.558111Z","iopub.status.idle":"2022-04-26T09:54:17.748461Z","shell.execute_reply.started":"2022-04-26T09:48:06.558071Z","shell.execute_reply":"2022-04-26T09:54:17.747756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape, data.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:17.749711Z","iopub.execute_input":"2022-04-26T09:54:17.749996Z","iopub.status.idle":"2022-04-26T09:54:17.758593Z","shell.execute_reply.started":"2022-04-26T09:54:17.749951Z","shell.execute_reply":"2022-04-26T09:54:17.757811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = ''\ntrainingDocs = list(data['processedText'])\nfor doc in trainingDocs:\n    texts = texts + doc\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:17.764595Z","iopub.execute_input":"2022-04-26T09:54:17.765156Z","iopub.status.idle":"2022-04-26T09:54:17.857495Z","shell.execute_reply.started":"2022-04-26T09:54:17.765114Z","shell.execute_reply":"2022-04-26T09:54:17.856783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(texts.split())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:17.858696Z","iopub.execute_input":"2022-04-26T09:54:17.859516Z","iopub.status.idle":"2022-04-26T09:54:19.445611Z","shell.execute_reply.started":"2022-04-26T09:54:17.859475Z","shell.execute_reply":"2022-04-26T09:54:19.444699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TorchVocab(object):\n    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n    Attributes:\n        freqs: A collections.Counter object holding the frequencies of tokens\n            in the data used to build the Vocab.\n        stoi: A collections.defaultdict instance mapping token strings to\n            numerical identifiers.\n        itos: A list of token strings indexed by their numerical identifiers.\n    \"\"\"\n\n    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n                 vectors=None, unk_init=None, vectors_cache=None):\n        \"\"\"Create a Vocab object from a collections.Counter.\n        Arguments:\n            counter: collections.Counter object holding the frequencies of\n                each value found in the data.\n            max_size: The maximum size of the vocabulary, or None for no\n                maximum. Default: None.\n            min_freq: The minimum frequency needed to include a token in the\n                vocabulary. Values less than 1 will be set to 1. Default: 1.\n            specials: The list of special tokens (e.g., padding or eos) that\n                will be prepended to the vocabulary in addition to an <unk>\n                token. Default: ['<pad>']\n            vectors: One of either the available pretrained vectors\n                or custom pretrained vectors (see Vocab.load_vectors);\n                or a list of aforementioned vectors\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n        \"\"\"\n        self.freqs = counter\n        counter = counter.copy()\n        min_freq = max(min_freq, 1)\n\n        self.itos = list(specials)\n        # frequencies of special tokens are not counted when building vocabulary\n        # in frequency order\n        for tok in specials:\n            del counter[tok]\n\n        max_size = None if max_size is None else max_size + len(self.itos)\n\n        # sort by frequency, then alphabetically\n        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n\n        for word, freq in words_and_frequencies:\n            if freq < min_freq or len(self.itos) == max_size:\n                break\n            self.itos.append(word)\n\n        # stoi is simply a reverse dict for itos\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n        self.vectors = None\n        if vectors is not None:\n            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n        else:\n            assert unk_init is None and vectors_cache is None\n\n    def __eq__(self, other):\n        if self.freqs != other.freqs:\n            return False\n        if self.stoi != other.stoi:\n            return False\n        if self.itos != other.itos:\n            return False\n        if self.vectors != other.vectors:\n            return False\n        return True\n\n    def __len__(self):\n        return len(self.itos)\n\n    def vocab_rerank(self):\n        self.stoi = {word: i for i, word in enumerate(self.itos)}\n\n    def extend(self, v, sort=False):\n        words = sorted(v.itos) if sort else v.itos\n        for w in words:\n            if w not in self.stoi:\n                self.itos.append(w)\n                self.stoi[w] = len(self.itos) - 1\n\n\nclass Vocab(TorchVocab):\n    def __init__(self, counter, max_size=None, min_freq=1):\n        self.pad_index = 0\n        self.unk_index = 1\n        self.eos_index = 2\n        self.sos_index = 3\n        self.mask_index = 4\n        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n                         max_size=max_size, min_freq=min_freq)\n\n    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n        pass\n\n    def from_seq(self, seq, join=False, with_pad=False):\n        pass\n\n    @staticmethod\n    def load_vocab(vocab_path: str) -> 'Vocab':\n        with open(vocab_path, \"rb\") as f:\n            return pickle.load(f)\n\n    def save_vocab(self, vocab_path):\n        with open(vocab_path, \"wb\") as f:\n            pickle.dump(self, f)\n\n\n# Building Vocab with text files\nclass WordVocab(Vocab):\n    def __init__(self, texts, max_size=None, min_freq=1):\n        print(\"Building Vocab\")\n        counter = Counter()\n        count = 0\n        words = ((texts.replace(\"\\n\", \"\").replace(\"\\t\", \"\")).replace(\"|\",\" \")).split()\n        for word in words:\n            if(not count % 10000):\n                print(word)\n            counter[word] += 1\n            count += 1\n        print(\"\\nTotal line\", count)\n        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n\n    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n        if isinstance(sentence, str):\n            sentence = sentence.split()\n\n        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n\n        if with_eos:\n            seq += [self.eos_index]  # this would be index 1\n        if with_sos:\n            seq = [self.sos_index] + seq\n\n        origin_seq_len = len(seq)\n\n        if seq_len is None:\n            pass\n        elif len(seq) <= seq_len:\n            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n        else:\n            seq = seq[:seq_len]\n\n        return (seq, origin_seq_len) if with_len else seq\n\n    def from_seq(self, seq, join=False, with_pad=False):\n        words = [self.itos[idx]\n                 if idx < len(self.itos)\n                 else \"<%d>\" % idx\n                 for idx in seq\n                 if not with_pad or idx != self.pad_index]\n\n        return \" \".join(words) if join else words\n\n    @staticmethod\n    def load_vocab(vocab_path: str) -> 'WordVocab':\n        with open(vocab_path, \"rb\") as f:\n            return pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:19.447098Z","iopub.execute_input":"2022-04-26T09:54:19.447527Z","iopub.status.idle":"2022-04-26T09:54:19.480628Z","shell.execute_reply.started":"2022-04-26T09:54:19.447481Z","shell.execute_reply":"2022-04-26T09:54:19.479787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab = WordVocab(texts,100000,1)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:19.48243Z","iopub.execute_input":"2022-04-26T09:54:19.482965Z","iopub.status.idle":"2022-04-26T09:54:29.527054Z","shell.execute_reply.started":"2022-04-26T09:54:19.482923Z","shell.execute_reply":"2022-04-26T09:54:29.526228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:29.528404Z","iopub.execute_input":"2022-04-26T09:54:29.528655Z","iopub.status.idle":"2022-04-26T09:54:30.940286Z","shell.execute_reply.started":"2022-04-26T09:54:29.528622Z","shell.execute_reply":"2022-04-26T09:54:30.939502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentEmbedding(nn.Embedding):\n    def __init__(self, embed_size=512):\n        super().__init__(3, embed_size, padding_idx=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.942871Z","iopub.execute_input":"2022-04-26T09:54:30.943393Z","iopub.status.idle":"2022-04-26T09:54:30.948228Z","shell.execute_reply.started":"2022-04-26T09:54:30.943351Z","shell.execute_reply":"2022-04-26T09:54:30.947361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.949843Z","iopub.execute_input":"2022-04-26T09:54:30.950179Z","iopub.status.idle":"2022-04-26T09:54:30.959664Z","shell.execute_reply.started":"2022-04-26T09:54:30.950098Z","shell.execute_reply":"2022-04-26T09:54:30.958787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenEmbedding(nn.Embedding):\n    def __init__(self, vocab_size, embed_size=512):\n        super().__init__(vocab_size, embed_size, padding_idx=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.961193Z","iopub.execute_input":"2022-04-26T09:54:30.961918Z","iopub.status.idle":"2022-04-26T09:54:30.968803Z","shell.execute_reply.started":"2022-04-26T09:54:30.961879Z","shell.execute_reply":"2022-04-26T09:54:30.967847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTEmbedding(nn.Module):\n    \"\"\"\n    BERT Embedding which is consisted with under features\n        1. TokenEmbedding : normal embedding matrix\n        2. PositionalEmbedding : adding positional information using sin, cos\n        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n\n        sum of all these features are output of BERTEmbedding\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_size, dropout=0.1):\n        \"\"\"\n        :param vocab_size: total vocab size\n        :param embed_size: embedding size of token embedding\n        :param dropout: dropout rate\n        \"\"\"\n        super().__init__()\n        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n        self.dropout = nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence, segment_label):\n        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.970651Z","iopub.execute_input":"2022-04-26T09:54:30.970973Z","iopub.status.idle":"2022-04-26T09:54:30.97979Z","shell.execute_reply.started":"2022-04-26T09:54:30.970932Z","shell.execute_reply":"2022-04-26T09:54:30.978939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.981648Z","iopub.execute_input":"2022-04-26T09:54:30.982164Z","iopub.status.idle":"2022-04-26T09:54:30.993207Z","shell.execute_reply.started":"2022-04-26T09:54:30.982124Z","shell.execute_reply":"2022-04-26T09:54:30.99225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = GELU()\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:30.994679Z","iopub.execute_input":"2022-04-26T09:54:30.995025Z","iopub.status.idle":"2022-04-26T09:54:31.003527Z","shell.execute_reply.started":"2022-04-26T09:54:30.994976Z","shell.execute_reply":"2022-04-26T09:54:31.002769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GELU(nn.Module):\n    \"\"\"\n    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n    \"\"\"\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.004723Z","iopub.execute_input":"2022-04-26T09:54:31.005152Z","iopub.status.idle":"2022-04-26T09:54:31.013534Z","shell.execute_reply.started":"2022-04-26T09:54:31.005113Z","shell.execute_reply":"2022-04-26T09:54:31.012767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.014902Z","iopub.execute_input":"2022-04-26T09:54:31.015426Z","iopub.status.idle":"2022-04-26T09:54:31.023228Z","shell.execute_reply.started":"2022-04-26T09:54:31.015376Z","shell.execute_reply":"2022-04-26T09:54:31.022298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"\n    Compute 'Scaled Dot Product Attention\n    \"\"\"\n\n    def forward(self, query, key, value, mask=None, dropout=None):\n        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n                 / math.sqrt(query.size(-1))\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        p_attn = F.softmax(scores, dim=-1)\n\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n\n        return torch.matmul(p_attn, value), p_attn","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.024499Z","iopub.execute_input":"2022-04-26T09:54:31.024942Z","iopub.status.idle":"2022-04-26T09:54:31.034521Z","shell.execute_reply.started":"2022-04-26T09:54:31.024901Z","shell.execute_reply":"2022-04-26T09:54:31.03377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    \"\"\"\n    Take in model size and number of heads.\n    \"\"\"\n\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n\n        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n        self.output_linear = nn.Linear(d_model, d_model)\n        self.attention = Attention()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n                             for l, x in zip(self.linear_layers, (query, key, value))]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n\n        return self.output_linear(x)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.036092Z","iopub.execute_input":"2022-04-26T09:54:31.036398Z","iopub.status.idle":"2022-04-26T09:54:31.049633Z","shell.execute_reply.started":"2022-04-26T09:54:31.036336Z","shell.execute_reply":"2022-04-26T09:54:31.048875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    Bidirectional Encoder = Transformer (self-attention)\n    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n    \"\"\"\n\n    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n        \"\"\"\n        :param hidden: hidden size of transformer\n        :param attn_heads: head sizes of multi-head attention\n        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, mask):\n        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n        x = self.output_sublayer(x, self.feed_forward)\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.050969Z","iopub.execute_input":"2022-04-26T09:54:31.05118Z","iopub.status.idle":"2022-04-26T09:54:31.062871Z","shell.execute_reply.started":"2022-04-26T09:54:31.05115Z","shell.execute_reply":"2022-04-26T09:54:31.062047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n    \"\"\"\n    BERT model : Bidirectional Encoder Representations from Transformers.\n    \"\"\"\n\n    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n        \"\"\"\n        :param vocab_size: vocab_size of total words\n        :param hidden: BERT model hidden size\n        :param n_layers: numbers of Transformer blocks(layers)\n        :param attn_heads: number of attention heads\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.hidden = hidden\n        self.n_layers = n_layers\n        self.attn_heads = attn_heads\n\n        # paper noted they used 4*hidden_size for ff_network_hidden_size\n        self.feed_forward_hidden = hidden * 4\n\n        # embedding for BERT, sum of positional, segment, token embeddings\n        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n\n        # multi-layers transformer blocks, deep network\n        self.transformer_blocks = nn.ModuleList(\n            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, segment_info):\n        # attention masking for padded token\n        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        x = self.embedding(x, segment_info)\n\n        # running over multiple transformer blocks\n        for transformer in self.transformer_blocks:\n            x = transformer.forward(x, mask)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:54:31.06452Z","iopub.execute_input":"2022-04-26T09:54:31.064865Z","iopub.status.idle":"2022-04-26T09:54:31.076126Z","shell.execute_reply.started":"2022-04-26T09:54:31.064823Z","shell.execute_reply":"2022-04-26T09:54:31.075314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert = BERT(len(newvocab), hidden=200, n_layers=8, attn_heads=5)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:35.005212Z","iopub.execute_input":"2022-04-26T09:57:35.005494Z","iopub.status.idle":"2022-04-26T09:57:35.105096Z","shell.execute_reply.started":"2022-04-26T09:57:35.005457Z","shell.execute_reply":"2022-04-26T09:57:35.104417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTLM(nn.Module):\n    \"\"\"\n    BERT Language Model\n    Next Sentence Prediction Model + Masked Language Model\n    \"\"\"\n\n    def __init__(self, bert: BERT, vocab_size):\n        \"\"\"\n        :param bert: BERT model which should be trained\n        :param vocab_size: total vocab size for masked_lm\n        \"\"\"\n\n        super().__init__()\n        self.bert = bert\n        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n\n    def forward(self, x, segment_label):\n        x = self.bert(x, segment_label)\n        return x#self.next_sentence(x), \n\n\nclass NextSentencePrediction(nn.Module):\n    \"\"\"\n    2-class classification model : is_next, is_not_next\n    \"\"\"\n\n    def __init__(self, hidden):\n        \"\"\"\n        :param hidden: BERT model output size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, 2)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x[:, 0]))\n\n\nclass MaskedLanguageModel(nn.Module):\n    \"\"\"\n    predicting origin token from masked input sequence\n    n-class classification problem, n-class = vocab_size\n    \"\"\"\n\n    def __init__(self, hidden, vocab_size):\n        \"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:39.914994Z","iopub.execute_input":"2022-04-26T09:57:39.9153Z","iopub.status.idle":"2022-04-26T09:57:39.929885Z","shell.execute_reply.started":"2022-04-26T09:57:39.915265Z","shell.execute_reply":"2022-04-26T09:57:39.928813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda_condition = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:40.883872Z","iopub.execute_input":"2022-04-26T09:57:40.884409Z","iopub.status.idle":"2022-04-26T09:57:40.888309Z","shell.execute_reply.started":"2022-04-26T09:57:40.884369Z","shell.execute_reply":"2022-04-26T09:57:40.887629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTLM(bert, len(newvocab)).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:42.327828Z","iopub.execute_input":"2022-04-26T09:57:42.328679Z","iopub.status.idle":"2022-04-26T09:57:42.489525Z","shell.execute_reply.started":"2022-04-26T09:57:42.328631Z","shell.execute_reply":"2022-04-26T09:57:42.488818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = BERTLM(bert, 36841).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:46.966661Z","iopub.execute_input":"2022-04-26T09:57:46.967397Z","iopub.status.idle":"2022-04-26T09:57:46.970719Z","shell.execute_reply.started":"2022-04-26T09:57:46.967356Z","shell.execute_reply":"2022-04-26T09:57:46.969904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/bert-pretrain-bothtask/45_model_weights.pth'))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T09:57:47.789614Z","iopub.execute_input":"2022-04-26T09:57:47.790151Z","iopub.status.idle":"2022-04-26T09:57:47.869688Z","shell.execute_reply.started":"2022-04-26T09:57:47.79011Z","shell.execute_reply":"2022-04-26T09:57:47.868812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:07:36.290304Z","iopub.execute_input":"2022-04-26T10:07:36.290561Z","iopub.status.idle":"2022-04-26T10:07:36.297521Z","shell.execute_reply.started":"2022-04-26T10:07:36.290532Z","shell.execute_reply":"2022-04-26T10:07:36.296482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list(model.named_parameters())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:03:51.528443Z","iopub.execute_input":"2022-04-26T10:03:51.529034Z","iopub.status.idle":"2022-04-26T10:03:51.532847Z","shell.execute_reply.started":"2022-04-26T10:03:51.528991Z","shell.execute_reply":"2022-04-26T10:03:51.531861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.transformer_blocks)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:03:45.020155Z","iopub.execute_input":"2022-04-26T10:03:45.020417Z","iopub.status.idle":"2022-04-26T10:03:45.043792Z","shell.execute_reply.started":"2022-04-26T10:03:45.020387Z","shell.execute_reply":"2022-04-26T10:03:45.0428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model.transformer_blocks:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-04-26T10:02:51.937239Z","iopub.execute_input":"2022-04-26T10:02:51.937518Z","iopub.status.idle":"2022-04-26T10:02:51.960125Z","shell.execute_reply.started":"2022-04-26T10:02:51.937486Z","shell.execute_reply":"2022-04-26T10:02:51.958963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:46.702481Z","iopub.execute_input":"2022-04-26T05:04:46.702748Z","iopub.status.idle":"2022-04-26T05:04:46.708259Z","shell.execute_reply.started":"2022-04-26T05:04:46.702707Z","shell.execute_reply":"2022-04-26T05:04:46.707415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mimicFilePath  =  '/kaggle/input/mimic-clean-text/MIMIC-III-Final_cleaned.csv'#os.path.join(dirname,'data\\MIMIC-III-Final_cleaned.csv')\nicdCodeFilepath  = '/kaggle/input/mimicdata/ICD_desc_with_freq.csv'#os.path.join(dirname,'data\\ICD_desc_with_freq.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:46.975172Z","iopub.execute_input":"2022-04-26T05:04:46.975424Z","iopub.status.idle":"2022-04-26T05:04:46.980777Z","shell.execute_reply.started":"2022-04-26T05:04:46.975397Z","shell.execute_reply":"2022-04-26T05:04:46.979965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parseArgs():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-mimicPath', required=False, default= 'mimicFilePath')\n    parser.add_argument('-icdPath', required=False, default = 'icdCodeFilepath')\n    parser.add_argument('-src_lang', required=False, default = 'en_core_web_sm')\n    #parser.add_argument('-trg_lang', required=False, default = 'fr')\n    parser.add_argument('-trainSize', required=False, default = 15000)\n    parser.add_argument('-valSize', required=False, default = 500)\n    parser.add_argument('-topCodes', required=False, default = 2833)\n    parser.add_argument('-min_freq', required=False, default = 2)\n    parser.add_argument('-no_cuda', action='store_true')\n    parser.add_argument('-SGDR', action='store_false')\n    parser.add_argument('-epochs', type=int, default=8)\n    parser.add_argument('-d_model', type=int, default=512)\n    parser.add_argument('-n_layers', type=int, default=6)\n    parser.add_argument('-heads', type=int, default=8)\n    parser.add_argument('-dropout', type=int, default=0.1)\n    parser.add_argument('-batchsize', type=int, default=1)\n    parser.add_argument('-printevery', type=int, default=100)\n    parser.add_argument('-lr', type=int, default=0.0001)\n    parser.add_argument('-load_weights')\n    parser.add_argument('-create_valset', action='store_true')\n    parser.add_argument('-max_strlen', type=int, default=80)\n    parser.add_argument('-floyd', action='store_false')\n    parser.add_argument('-checkpoint', type=int, default=1)\n    opt = parser.parse_args(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:47.406614Z","iopub.execute_input":"2022-04-26T05:04:47.407124Z","iopub.status.idle":"2022-04-26T05:04:47.419912Z","shell.execute_reply.started":"2022-04-26T05:04:47.407075Z","shell.execute_reply":"2022-04-26T05:04:47.418941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = parseArgs()\nopt","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:48.105058Z","iopub.execute_input":"2022-04-26T05:04:48.105324Z","iopub.status.idle":"2022-04-26T05:04:48.11077Z","shell.execute_reply.started":"2022-04-26T05:04:48.105295Z","shell.execute_reply":"2022-04-26T05:04:48.110062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mimicFilePath  =  '/kaggle/input/mimic-clean-text/MIMIC-III-Final_cleaned.csv'#os.path.join(dirname,'data\\MIMIC-III-Final_cleaned.csv')\nicdCodeFilepath  = '/kaggle/input/mimicdata/ICD_desc_with_freq.csv'#os.path.join(dirname,'data\\ICD_desc_with_freq.csv')\n#embedFilePath = '/kaggle/input/pmcmodel/PMC-w2v.bin' #os.path.join(dirname,'data\\pmcModel\\PMC-w2v.bin')\n#vocabFilePath = '/kaggle/input/autoencoderdecoderonpmcembeddingvocab/Vocab.txt' #os.path.join(dirname, 'data\\Vocab.txt')\n#opt = parseArgs()\n\n#opt.vocabFile  = vocabFilePath\n#opt.embedFile = embedFilePath","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:48.601263Z","iopub.execute_input":"2022-04-26T05:04:48.601782Z","iopub.status.idle":"2022-04-26T05:04:48.605922Z","shell.execute_reply.started":"2022-04-26T05:04:48.601742Z","shell.execute_reply":"2022-04-26T05:04:48.605225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#opt.mimicPath \n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:49.214228Z","iopub.execute_input":"2022-04-26T05:04:49.214682Z","iopub.status.idle":"2022-04-26T05:04:49.219357Z","shell.execute_reply.started":"2022-04-26T05:04:49.214646Z","shell.execute_reply":"2022-04-26T05:04:49.21864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport spacy\n#from Tokenize import tokenize\nfrom collections import Counter\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef CreateCodeDescpMap(opt):\n\t#mimicData = pd.read_csv(opt.mimicPath)[:opt.trainSize]\n\ticdCodes = pd.read_csv(icdCodeFilepath)[:2833]\n\ticdCodeDescDict = {}\n\tcodes  = list(icdCodes[\"Code\"])\n\tDesc  = list(icdCodes[\"Long Description\"])\n\tcode2Index = {}\n\tfor i , code in enumerate(codes):\n\t\ticdCodeDescDict[code] = Desc[i]\n\t\tcode2Index[code] = i\n\treturn icdCodeDescDict,code2Index\n\ndef CreateTextFileLableMap(opt):\n\tmimicData = pd.read_csv(mimicFilePath)[:150]\n\ticdCodeDescDict,code2Index = CreateCodeDescpMap(opt)\n\ttrueLabels = list(mimicData['ICD9_CODE'])\n\ttextFileLabel = {}\n\tfor i , trueLabel in enumerate(trueLabels):\n\t\ttextFileLabel[i+1]  = [y for y in trueLabel[2:-2].split(\"', '\")if y in  icdCodeDescDict]\n\treturn textFileLabel,icdCodeDescDict,code2Index\n\n\ndef processList(opt, tokensLists):\n\tvocabDict = {}\n\tVocabDictFinal  = {'<UNK>':1,'<PAD>':0}\n\tfor i, tokensList in enumerate(tokensLists):\n\t\tif(not i%1000):\n\t\t\tprint(\"\\ndoc processed \",i)\n\t\tfor token in tokensList:\n\t\t\tif token in vocabDict:\n\t\t\t\tcount  = vocabDict[token]\n\t\t\t\tvocabDict[token] = count + 1\n\t\t\telse:\n\t\t\t\tvocabDict[token] = 1\n\tminimumTokenCountInVocab  = opt.min_freq\n\tfor token, count in vocabDict.items():\n\t\tif(count>= minimumTokenCountInVocab and token not in VocabDictFinal):\n\t\t\tVocabDictFinal[token] = len(VocabDictFinal)\n\treturn VocabDictFinal\n\t\n\t\n\t\ndef CreateVocab(opt,tokensLists):\n\tprint(len(tokensLists))\n\tminimumTokenCountInVocab  = opt.min_freq\n\tallTokensLost = []\n\tfor i, tokensList in enumerate(tokensLists):\n\t\tif(not i%1000):\n\t\t\tprint(\"Processed documents\",(i))\n\t\tallTokensLost = allTokensLost+tokensList\n\ttokenCountDict = Counter(allTokensLost)\n\tprint(len(allTokensLost),len(tokenCountDict))\n\tVocabDict  = {'<UNK>':1,'<PAD>':2}\n\t#VocabDict = {VocabDict[token] = len(VocabDict)+1 for token , count in tokenCountDict.items() if count>=minimumTokenCountInVocab}\n\tfor token, count in tokenCountDict.items():\n\t\tif(count>= minimumTokenCountInVocab and token not in VocabDict):\n\t\t\tVocabDict[token] = len(VocabDict)+1\n\treturn VocabDict\n\ndef CreateVocabOnMimicTrainCodeDesc(opt, icdCodeDescDict):\n\ttok = tokenize(opt)\n\tmimicData = pd.read_csv(opt.mimicPath)[:opt.trainSize]\n\tmimicTexts = list(mimicData['clean_text'])\n\tmimicTokensList  = [tok.tokenizer(texts) for texts in mimicTexts ]\n\ticdCodesTokenList = [tok.tokenizer(codeTexts) for code,codeTexts in icdCodeDescDict.items()]\n\tprint(len(mimicTokensList),len(icdCodesTokenList))\n\t#return (CreateVocab(opt,(mimicTokensList+icdCodesTokenList)))\n\treturn (processList(opt,(mimicTokensList+icdCodesTokenList)))\n\t\ndef generateTensor(opt,tok, texts, vocabToIndexDict):\n\ttextTokens  = tok.tokenizer(texts)\n\ttextTokensToIndex  = [vocabToIndexDict[token] if token in vocabToIndexDict else vocabToIndexDict['<UNK>'] for token in textTokens ]\n\treturn torch.tensor([textTokensToIndex])\n\ndef generateTensorForTrain(opt,tok, texts, vocabToIndexDict):\n\ttextTokens  = tok.tokenizer(texts)\n\ttextTokensToIndex  = [vocabToIndexDict[token] if token in vocabToIndexDict else vocabToIndexDict['<UNK>'] for token in textTokens ]\n\tprint(\"\\nchecking the shape of the train tensor\",torch.tensor(textTokensToIndex).shape)\n\treturn torch.tensor(textTokensToIndex)\n\n\ndef findMaxLength(mimicTextsTrainData):\n\tmaxLen = 0\n\tfor texts in mimicTextsTrainData:\n\t\tif(len(texts)>maxLen):\n\t\t\tmaxlen = len(texts)\n\treturn maxLen\ndef padTrainData(opt, mimicTextsTrainData,tok, vocabToIndexDict):\n\ttrainTensor = []\n\tbatchsize = opt.batchsize\n\tdocIndex = 0\n\tbatchDocs = []\n\twhile(docIndex < opt.trainSize):\n\t\tbatchDocs.append(generateTensorForTrain(opt,tok, mimicTextsTrainData[docIndex], vocabToIndexDict))\n\t\tif(len(batchDocs) == batchsize or (docIndex + 1== opt.trainSize)):\n\t\t\ttrainTensor.append(torch.nn.utils.rnn.pad_sequence(batchDocs, batch_first=True))\n\t\t\tbatchDocs = []\n\t\t\tdocIndex += 1\n\t\telse:\n\t\t\tdocIndex += 1\n\treturn trainTensor\n\ndef processMimicTexts(opt,mimicTextsTrainData,tok,vocabToIndexDict):\n    trainTensor = []\n    print(\"\\nTrain data size is\",len(mimicTextsTrainData))\n    for texts in mimicTextsTrainData:\n        mimicTokens  = tok.tokenizer(texts)\n        mimicTokens  = [vocabToIndexDict[token] if token in vocabToIndexDict else vocabToIndexDict['<UNK>'] for token in mimicTokens ]\n        mimicTokens = [mimicTokens[i: i+512] for i in range(0, len(mimicTokens), 512)]\n        finalList =[torch.tensor(i) for  i in mimicTokens]\n        finalList = pad_sequence(finalList, batch_first=True, padding_value=0)\n        trainTensor.append(finalList)\n    print(\"\\nTraining data prepared\")\n    return trainTensor\n    \n\ndef create_dataset(opt, vocabToIndexDict, icdCodeDescDict):\n\ttok = tokenize(opt)\n\tmimicData = pd.read_csv(opt.mimicPath)[:opt.trainSize]\n\tmimicTextsTrainData= list((mimicData[:opt.trainSize])['clean_text'])\n\tmimicTextsValData= list((mimicData[:opt.valSize])['clean_text'])\n\ticdCodes = pd.read_csv(opt.icdPath)[:opt.topCodes]\n\tDesc  = list(icdCodes[\"Long Description\"])\n\t#maxLen = findMaxLength(mimicTextsTrainData)\n\t#trainOnMimicData = [generateTensor(opt,tok,texts,vocabToIndexDict) for texts in mimicTextsTrainData]\n\t#trainOnMimicData = padTrainData(opt,mimicTextsTrainData,tok,vocabToIndexDict)\n\ttrainOnMimicData = processMimicTexts(opt,mimicTextsTrainData,tok,vocabToIndexDict)\n\tvalOnMimicData = [generateTensor(opt,tok,texts,vocabToIndexDict) for texts in mimicTextsValData]\n\ttrainOncodeDesc = [generateTensor(opt,tok,texts,vocabToIndexDict) for texts in Desc]\n\treturn trainOnMimicData,valOnMimicData,trainOncodeDesc","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:49.709567Z","iopub.execute_input":"2022-04-26T05:04:49.71Z","iopub.status.idle":"2022-04-26T05:04:54.525504Z","shell.execute_reply.started":"2022-04-26T05:04:49.709952Z","shell.execute_reply":"2022-04-26T05:04:54.524777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textFileLabel,icdCodeDescDict, code2Index = CreateTextFileLableMap(opt)\n#opt.vocabToIndexDict, opt.indexToVocabDict = createVocab(opt,icdCodeDescDict)\n#print(len(opt.indexToVocabDict))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:04:54.527155Z","iopub.execute_input":"2022-04-26T05:04:54.52739Z","iopub.status.idle":"2022-04-26T05:05:02.67051Z","shell.execute_reply.started":"2022-04-26T05:04:54.527356Z","shell.execute_reply":"2022-04-26T05:05:02.66977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(newvocab.stoi)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.671675Z","iopub.execute_input":"2022-04-26T05:05:02.672117Z","iopub.status.idle":"2022-04-26T05:05:02.677563Z","shell.execute_reply.started":"2022-04-26T05:05:02.672079Z","shell.execute_reply":"2022-04-26T05:05:02.676916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processTextIntoSentences(data['processedText'][0])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.679689Z","iopub.execute_input":"2022-04-26T05:05:02.680103Z","iopub.status.idle":"2022-04-26T05:05:02.706694Z","shell.execute_reply.started":"2022-04-26T05:05:02.680069Z","shell.execute_reply":"2022-04-26T05:05:02.706042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.708136Z","iopub.execute_input":"2022-04-26T05:05:02.708626Z","iopub.status.idle":"2022-04-26T05:05:02.712275Z","shell.execute_reply.started":"2022-04-26T05:05:02.708591Z","shell.execute_reply":"2022-04-26T05:05:02.711354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"te = data['processedText'][0]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.713829Z","iopub.execute_input":"2022-04-26T05:05:02.714426Z","iopub.status.idle":"2022-04-26T05:05:02.720169Z","shell.execute_reply.started":"2022-04-26T05:05:02.714389Z","shell.execute_reply":"2022-04-26T05:05:02.71945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport tqdm\nimport torch\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.721548Z","iopub.execute_input":"2022-04-26T05:05:02.722039Z","iopub.status.idle":"2022-04-26T05:05:02.729385Z","shell.execute_reply.started":"2022-04-26T05:05:02.722005Z","shell.execute_reply":"2022-04-26T05:05:02.728692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self,vocab, seq_len, list_IDs, labels, encoding=\"utf-8\", on_memory=True):\n        self.vocab = vocab\n        self.seq_len = seq_len\n\n        self.on_memory = on_memory\n        #self.corpus_lines = corpus_lines\n        self.list_IDs  = list_IDs \n        self.encoding = encoding\n        self.labels = labels\n        self.texts  = data['processedText']\n\n\n    def __len__(self):\n        return len(self.list_IDs)\n\n    def __getitem__(self, item):# if item count start from 1 and till the size of docuemts length \n        text_ = self.texts[item-1]\n        text_ = text_.split('|')\n        if(len(text_) % 2):\n            text_.append(text_[-1])\n        text_ = [text_[i: i+2] for i in range(0, len(text_), 2)]\n        #print(len(text_))\n        bert_input_ = self.generateTensor(text_)\n        return bert_input_\n        #return {key: torch.tensor(value) for key, value in output.items()}\n    def generateTensor(self, text):\n        numberOfLines  = len(text)\n        bert_input_ = []\n        bert_label_ = []\n        segment_label_ = []\n        is_next_ = []\n        for i in range(numberOfLines):\n            t1, t2, is_next_label = self.random_sent(i, text)#generate the label for next sentence prediction in the form of 0 or 1\n            t1_random, t1_label = self.random_word(t1)#This will generate the masked token for sentence A and the label index for the masked token \n            t2_random, t2_label = self.random_word(t2)\n\n        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n            t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n            t2 = t2_random + [self.vocab.eos_index]\n\n            t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n            t2_label = t2_label + [self.vocab.pad_index]\n\n            segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n            bert_input = (t1 + t2)[:self.seq_len]\n            bert_label = (t1_label + t2_label)[:self.seq_len]\n\n            padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]#This would pad the input, output lable and sengement label up to the maximum length\n            bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n            bert_input_.append(bert_input)\n            bert_label_.append(bert_label)\n            segment_label_.append(segment_label)\n            is_next_.append(is_next_label)\n        return ([torch.tensor(bert_input_),torch.tensor(bert_label_),torch.tensor(segment_label_), torch.tensor(is_next_)])\n    def random_word(self, sentence):\n        tokens = sentence.split()\n        output_label = []\n\n        for i, token in enumerate(tokens):\n            prob = random.random()\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    tokens[i] = self.vocab.mask_index\n\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    tokens[i] = random.randrange(len(self.vocab))\n\n                # 10% randomly change token to current token\n                else:\n                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n\n                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n\n            else:\n                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n                output_label.append(0)\n\n        return tokens, output_label\n\n    def random_sent(self, index,text):\n        t1, t2 = self.get_corpus_line(index,text)\n\n        # output_text, label(isNotNext:0, isNext:1)\n        if random.random() > 0.5:\n            return t1, t2, 1\n        else:\n            return t1, self.get_random_line(text), 0\n\n    def get_corpus_line(self, item,text):\n        if self.on_memory:\n            return text[item][0], text[item][1]\n        \"\"\"else:\n            line = self.file.__next__()\n            if line is None:\n                self.file.close()\n                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n                line = self.file.__next__()\n\n            t1, t2 = line[:-1].split(\"\\t\")\n            return t1, t2\"\"\"\n\n    def get_random_line(self,text):\n        if self.on_memory:\n            return text[random.randrange(len(text))][1]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.730956Z","iopub.execute_input":"2022-04-26T05:05:02.731432Z","iopub.status.idle":"2022-04-26T05:05:02.755405Z","shell.execute_reply.started":"2022-04-26T05:05:02.731398Z","shell.execute_reply":"2022-04-26T05:05:02.754571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listIds = [i for i in range(data.shape[0])]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.756539Z","iopub.execute_input":"2022-04-26T05:05:02.757923Z","iopub.status.idle":"2022-04-26T05:05:02.767133Z","shell.execute_reply.started":"2022-04-26T05:05:02.757886Z","shell.execute_reply":"2022-04-26T05:05:02.766441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#textFileLabel","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.769511Z","iopub.execute_input":"2022-04-26T05:05:02.770632Z","iopub.status.idle":"2022-04-26T05:05:02.776982Z","shell.execute_reply.started":"2022-04-26T05:05:02.7706Z","shell.execute_reply":"2022-04-26T05:05:02.77613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testDataSet = BERTDataset(newvocab,256, listIds, textFileLabel)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.778097Z","iopub.execute_input":"2022-04-26T05:05:02.778747Z","iopub.status.idle":"2022-04-26T05:05:02.785094Z","shell.execute_reply.started":"2022-04-26T05:05:02.778691Z","shell.execute_reply":"2022-04-26T05:05:02.784433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testDataSet[1][0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.786288Z","iopub.execute_input":"2022-04-26T05:05:02.787109Z","iopub.status.idle":"2022-04-26T05:05:02.797707Z","shell.execute_reply.started":"2022-04-26T05:05:02.787042Z","shell.execute_reply":"2022-04-26T05:05:02.797027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainTensors = testDataSet[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.799062Z","iopub.execute_input":"2022-04-26T05:05:02.800005Z","iopub.status.idle":"2022-04-26T05:05:02.805883Z","shell.execute_reply.started":"2022-04-26T05:05:02.799942Z","shell.execute_reply":"2022-04-26T05:05:02.805288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a, b, c = model(trainTensors[0].to(device), trainTensors[2].to(device))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.808553Z","iopub.execute_input":"2022-04-26T05:05:02.808929Z","iopub.status.idle":"2022-04-26T05:05:02.814184Z","shell.execute_reply.started":"2022-04-26T05:05:02.80889Z","shell.execute_reply":"2022-04-26T05:05:02.813503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.81544Z","iopub.execute_input":"2022-04-26T05:05:02.816086Z","iopub.status.idle":"2022-04-26T05:05:02.822134Z","shell.execute_reply.started":"2022-04-26T05:05:02.816051Z","shell.execute_reply":"2022-04-26T05:05:02.821273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab.stoi.get('<unk>',newvocab.unk_index )","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.823438Z","iopub.execute_input":"2022-04-26T05:05:02.824328Z","iopub.status.idle":"2022-04-26T05:05:02.831769Z","shell.execute_reply.started":"2022-04-26T05:05:02.824294Z","shell.execute_reply":"2022-04-26T05:05:02.830987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generateTensorsOnTextData(text):\n    #text = text.split()\n    sentenceSize  = 125\n    line = convert_to_unicode(text)\n    line  = cleanText(text)\n    tokens = tokenizer.tokenize(line)\n    #print(\"\\ntotal token count \", len(tokens))\n    padTokens  = []\n    all_tokens = [tokens[i: i+sentenceSize] for i in range(0, len(tokens), sentenceSize)]\n    if(len(all_tokens) == 1):\n        padTokens.append(\" \".join(all_tokens[0]))\n        padTokens.append(\" \".join([\"<pad>\" for _ in range(sentenceSize)]))\n        #return padTokens\n    else:\n        if(not (len(all_tokens)% 2)):\n            padTokens = [\" \".join(tok) for tok in all_tokens]\n            #return padTokens\n        else:\n            padTokens = [\" \".join(tok) for tok in all_tokens]\n            padTokens.append(\" \".join([\"<pad>\" for _ in range(sentenceSize)]))\n            #return padTokens\n    text = [padTokens[i: i+2] for i in range(0, len(padTokens), 2)]\n    bert_input_ = []\n    bert_label_ = []\n    segment_label_ = []\n    is_next_ = []\n    for i in range(len(text)):\n            t1, t2, is_next_label = text[i][0], text[i][1], 1#(i, text)#generate the label for next sentence prediction in the form of 0 or 1\n            t1 = [newvocab.stoi.get(tko, newvocab.unk_index) for tko in t1.split()]\n            t2 = [newvocab.stoi.get(tko, newvocab.unk_index) for tko in t2.split()]\n\n        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n            t1 = [newvocab.sos_index] + t1 + [newvocab.eos_index]\n            t2 = t2 + [newvocab.eos_index]\n            segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:256]\n            bert_input = (t1 + t2)[:256]\n            padding = [newvocab.pad_index for _ in range(256 - len(bert_input))]#This would pad the input, output lable and sengement label up to the maximum length\n            bert_input.extend(padding), segment_label.extend(padding)\n            bert_input_.append(bert_input)\n            segment_label_.append(segment_label)\n    return torch.tensor(bert_input_), torch.tensor(segment_label_)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.833846Z","iopub.execute_input":"2022-04-26T05:05:02.834339Z","iopub.status.idle":"2022-04-26T05:05:02.84877Z","shell.execute_reply.started":"2022-04-26T05:05:02.83422Z","shell.execute_reply":"2022-04-26T05:05:02.848046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generateTensorsOnTextData(te)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:02.852635Z","iopub.execute_input":"2022-04-26T05:05:02.853405Z","iopub.status.idle":"2022-04-26T05:05:02.858682Z","shell.execute_reply.started":"2022-04-26T05:05:02.85337Z","shell.execute_reply":"2022-04-26T05:05:02.857966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab.stoi.get('<pad>', newvocab.unk_index)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:04.098535Z","iopub.execute_input":"2022-04-26T05:05:04.09936Z","iopub.status.idle":"2022-04-26T05:05:04.10566Z","shell.execute_reply.started":"2022-04-26T05:05:04.099319Z","shell.execute_reply":"2022-04-26T05:05:04.104878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_embedding(text):\n    a, b = generateTensorsOnTextData(text)\n    embeddings = []\n    #tensors = testDataSet[docNumber]\n    c = model(a.to(device), b.to(device))\n    c = c[:,1:,:]\n    #print(c.shape)\n    return c.contiguous().view(-1,200)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:19.606485Z","iopub.execute_input":"2022-04-26T05:05:19.606758Z","iopub.status.idle":"2022-04-26T05:05:19.6122Z","shell.execute_reply.started":"2022-04-26T05:05:19.606709Z","shell.execute_reply":"2022-04-26T05:05:19.611528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_word_embedding(\"this is the string\").shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:05:23.179648Z","iopub.execute_input":"2022-04-26T05:05:23.180205Z","iopub.status.idle":"2022-04-26T05:05:26.485201Z","shell.execute_reply.started":"2022-04-26T05:05:23.180168Z","shell.execute_reply":"2022-04-26T05:05:26.484468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence_embedding(sent):\n    #words = tokenizer.tokenize(sent)\n    #m = len(words)\n    total_embedding = 0\n    \n    #Get wordwise Embeddings as tensors\n    #sent_embedding = np.array([get_word_embedding(word, embedding) for word in words])\n    sent_embedding = sent#.cpu().detach().numpy()#np.array([word.detach().to('cpu').numpy() for word in get_word_embedding(sent)])\n    \n    #Get the embedding of sentence by adding embedding tensors\n    total_embedding = np.sum(sent_embedding, axis = 0)\n    \n    try:\n        return total_embedding/np.linalg.norm(total_embedding, axis = 0, keepdims=True)\n    except:\n        #print('sent : ', sent)\n        #print('m: ', m)\n        return np.array(total_embedding/np.linalg.norm(total_embedding, axis = 0, keepdims=True))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:25.927674Z","iopub.execute_input":"2022-04-26T05:06:25.927967Z","iopub.status.idle":"2022-04-26T05:06:25.934612Z","shell.execute_reply.started":"2022-04-26T05:06:25.927937Z","shell.execute_reply":"2022-04-26T05:06:25.933592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get_sentence_embedding(get_word_embedding(1)).shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:29.384649Z","iopub.execute_input":"2022-04-26T05:06:29.385161Z","iopub.status.idle":"2022-04-26T05:06:29.388601Z","shell.execute_reply.started":"2022-04-26T05:06:29.385121Z","shell.execute_reply":"2022-04-26T05:06:29.387659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ICD_desc_filename = ICD_DESC_FILENAME\ndf_icd = pd.read_csv(ICD_desc_filename, encoding = 'latin1')\n\ndf_icd = df_icd[['Code', 'Long Description']]\n\nprint(df_icd.shape)\n\nprint('\\n', df_icd.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:29.70136Z","iopub.execute_input":"2022-04-26T05:06:29.70167Z","iopub.status.idle":"2022-04-26T05:06:29.734501Z","shell.execute_reply.started":"2022-04-26T05:06:29.701634Z","shell.execute_reply":"2022-04-26T05:06:29.733787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==3.0.0\nimport os \nimport sys\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pad_sequence\nimport time\nimport random\nimport math\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom keras.models import Model, load_model\nfrom keras.layers import LSTM, Dense, TimeDistributed, Input, Masking, RepeatVector, Bidirectional, Embedding\nnltk.download('punkt')\nimport string\npunc = list(string.punctuation)\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = list(stopwords.words('english'))\nprint(stop_words)\nfrom sklearn.metrics import pairwise\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow as tf\nimport pickle\nimport csv\nimport os\nfrom scipy.spatial.distance import cosine\nimport time\nfrom sklearn.feature_extraction.text import TfidfVectorizer as tfidf\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:30.257415Z","iopub.execute_input":"2022-04-26T05:06:30.257657Z","iopub.status.idle":"2022-04-26T05:06:44.78507Z","shell.execute_reply.started":"2022-04-26T05:06:30.25763Z","shell.execute_reply":"2022-04-26T05:06:44.783519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_icd_cleaned = df_icd\n\n#Delete df_icd\ndel df_icd\n\ndef clean_icd(x):\n    sent = x\n    sent_tokenized = word_tokenize(sent)\n    \n    clean_sent = ''\n    \n    for token in sent_tokenized:\n        if token.lower() not in punc and token.lower() not in stop_words:\n            clean_sent = clean_sent + ' ' + token.lower()\n    \n    x = clean_sent\n    \n    del clean_sent\n    \n    return x\n\n\ndf_icd_cleaned['Long Description'] = df_icd_cleaned.apply(lambda x: clean_icd(x['Long Description']), axis = 1)\n\ndf_icd_cleaned.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:44.787334Z","iopub.execute_input":"2022-04-26T05:06:44.787609Z","iopub.status.idle":"2022-04-26T05:06:45.921328Z","shell.execute_reply.started":"2022-04-26T05:06:44.787573Z","shell.execute_reply":"2022-04-26T05:06:45.920598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icd_sentences = list(df_icd_cleaned.head(NUM_ICD_CODES)['Long Description']) # Number of sentences\nicd_codes = list(df_icd_cleaned.head(NUM_ICD_CODES)['Code'])\nicd_index = list(range(NUM_ICD_CODES))\n\nprint(len(icd_sentences), len(icd_codes), len(icd_index))\nprint(\"ICD_Sentences = \", len(icd_sentences))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:45.922626Z","iopub.execute_input":"2022-04-26T05:06:45.922903Z","iopub.status.idle":"2022-04-26T05:06:45.932042Z","shell.execute_reply.started":"2022-04-26T05:06:45.92287Z","shell.execute_reply":"2022-04-26T05:06:45.931159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3 Dictionary of ICD index, ICD Code and ICD desc\n\n# index, codes\ndict_index_code = dict(zip(icd_index, icd_codes))\n\n# codes, index\ndict_code_index = dict(zip(icd_codes, icd_index))\n\n#index, code desc\ndict_index_desc = dict(zip(icd_index, icd_sentences))\n\n#icd sentence embedding matrix slice\n#icd_sent_emb_matrix = icd_sent_emb_matrix[:NUM_ICD_CODES]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:45.934154Z","iopub.execute_input":"2022-04-26T05:06:45.934402Z","iopub.status.idle":"2022-04-26T05:06:45.942555Z","shell.execute_reply.started":"2022-04-26T05:06:45.934369Z","shell.execute_reply":"2022-04-26T05:06:45.941665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_index_code[500]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:06:45.943436Z","iopub.execute_input":"2022-04-26T05:06:45.943647Z","iopub.status.idle":"2022-04-26T05:06:45.953206Z","shell.execute_reply.started":"2022-04-26T05:06:45.943625Z","shell.execute_reply":"2022-04-26T05:06:45.952401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DD_filename = DD_FILENAME\ndf_DD = pd.read_csv(DD_filename)\ndf_DD = df_DD[0:50]\ndf_DD.reset_index(drop=True, inplace= True)\nFILESIZE = df_DD.shape[0]\ndf_DD.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:00.545487Z","iopub.execute_input":"2022-04-26T05:07:00.545782Z","iopub.status.idle":"2022-04-26T05:07:09.081439Z","shell.execute_reply.started":"2022-04-26T05:07:00.545748Z","shell.execute_reply":"2022-04-26T05:07:09.080616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom spacy.language import Language\n\n@Language.component(\"custom_boundary\")\ndef set_custom_boundaries(doc):\n    for token in doc[:-1]:\n        if token.text == '...' or token.text == '.' or token.text =='!' :\n            doc[token.i+1].is_sent_start = True\n    return doc\n\ncustom_nlp = spacy.load('en_core_web_sm')\n\ncustom_nlp.add_pipe(\"custom_boundary\", before='parser')\n\ndef removingNegativeSentence(text):\n    cleanText = []\n    negative_token = [\"not\",\"absence\",\"no\"]\n    custom_doc = custom_nlp(text)\n    custom_sentences = list(custom_doc.sents)\n    for sentence in custom_sentences:\n        #print (sentence.text)\n        flag  = False\n        for token in sentence:\n            #print (type(token))\n            if token.text in negative_token:\n                #Print the Dependant Word to Neg words\n                #print (token.text, token.tag_, token.head.text, token.dep_)\n                #displacy.serve(sentence, style='dep')\n                flag  = True\n                break\n        if(not flag):\n            #print(sentence.text)\n            cleanText.append(sentence.text)\n    return (' '.join(cleanText))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:09.083016Z","iopub.execute_input":"2022-04-26T05:07:09.083349Z","iopub.status.idle":"2022-04-26T05:07:09.793009Z","shell.execute_reply.started":"2022-04-26T05:07:09.083311Z","shell.execute_reply":"2022-04-26T05:07:09.792251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DD_Data = df_DD[['ICD9_CODE', 'clean_text']]\ndel df_DD\nDD_Data","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:09.794245Z","iopub.execute_input":"2022-04-26T05:07:09.794671Z","iopub.status.idle":"2022-04-26T05:07:09.811291Z","shell.execute_reply.started":"2022-04-26T05:07:09.794632Z","shell.execute_reply":"2022-04-26T05:07:09.810614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Negation Removal\nDD_Data['clean_text'] = DD_Data['clean_text'].apply(lambda x: processTextIntoSentences(removingNegativeSentence(x)))\nDD_Data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:14.943744Z","iopub.execute_input":"2022-04-26T05:07:14.944005Z","iopub.status.idle":"2022-04-26T05:07:25.314089Z","shell.execute_reply.started":"2022-04-26T05:07:14.943976Z","shell.execute_reply":"2022-04-26T05:07:25.313395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tf_modified(docs):\n    freqDict = {}\n    for doc in docs:\n        tokens  = tokenizer.tokenize(doc)\n        for token in tokens:\n            if(token in freqDict):\n                freqDict[token] = int(freqDict[token]) + 1\n            else:\n                freqDict[token] = 1\n    freq_vec = pd.DataFrame(freqDict.values(), index=freqDict.keys(), columns=['Frequency'])\n    return  freq_vec, freqDict","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:25.315548Z","iopub.execute_input":"2022-04-26T05:07:25.315812Z","iopub.status.idle":"2022-04-26T05:07:25.323782Z","shell.execute_reply.started":"2022-04-26T05:07:25.315777Z","shell.execute_reply":"2022-04-26T05:07:25.323071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tf(docs):\n    \n    vectorizer = CountVectorizer()#CountVectorizer(stop_words = 'english')\n    X = vectorizer.fit_transform(docs)#(docs['sentence'])\n\n    # place tf values in a pandas data frame\n    tf_vecs = pd.DataFrame(X.T.todense(), index = vectorizer.get_feature_names())\n    \n    #Take sum accross sentences\n    freq_vec = tf_vecs.sum(axis = 1)\n    freq_vec = pd.DataFrame(freq_vec, index=freq_vec.index, columns=['Frequency'])\n    \n    return vectorizer, freq_vec","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:28.329042Z","iopub.execute_input":"2022-04-26T05:07:28.329315Z","iopub.status.idle":"2022-04-26T05:07:28.335029Z","shell.execute_reply.started":"2022-04-26T05:07:28.329277Z","shell.execute_reply":"2022-04-26T05:07:28.334301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_unwanted_words(words, freq_vec):\n    \n    pruned_words = [x for x in words if x in freq_vec.index]\n    \n    #for i in words:\n    #    if i in freq_vec.index:\n    #        pruned_words.append(i)\n    \n    return pruned_words\n    #return list(set(pruned_words))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:29.108931Z","iopub.execute_input":"2022-04-26T05:07:29.109518Z","iopub.status.idle":"2022-04-26T05:07:29.114037Z","shell.execute_reply.started":"2022-04-26T05:07:29.109478Z","shell.execute_reply":"2022-04-26T05:07:29.113329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.tokenize(mimicTexts)\ndef get_tf_vector(sent, freq_vec):\n    #tokenize sentence\n    words = list(set(tokenizer.tokenize(sent)))\n    \n    #remove terms not occuring in freq_vec\n    pruned_words = remove_unwanted_words(words, freq_vec)\n    \n    \n    #get the slice of freq_vec using the remaining terms\n    #print(words, pruned_words)\n    freq_slice = freq_vec.loc[pruned_words, :]\n    \n    #L1 normalize to obtain tf vector\n    tf_vec = freq_slice/np.linalg.norm(freq_slice, ord = 1)    \n\n    return tf_vec, pruned_words","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:30.244558Z","iopub.execute_input":"2022-04-26T05:07:30.245287Z","iopub.status.idle":"2022-04-26T05:07:30.251124Z","shell.execute_reply.started":"2022-04-26T05:07:30.245236Z","shell.execute_reply":"2022-04-26T05:07:30.249761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tf_vector_modified(sent, freq_vec):\n    #tokenize sentence\n    words = (tokenizer.tokenize(sent))\n    \n    #remove terms not occuring in freq_vec\n    pruned_words = remove_unwanted_words(words, freq_vec)\n    \n    \n    #get the slice of freq_vec using the remaining terms\n    #print(words, pruned_words)\n    freq_slice = freq_vec.loc[pruned_words, :]\n    \n    #L1 normalize to obtain tf vector\n    tf_vec = freq_slice/np.linalg.norm(freq_slice, ord = 1)    \n\n    return tf_vec, pruned_words","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:30.65726Z","iopub.execute_input":"2022-04-26T05:07:30.65752Z","iopub.status.idle":"2022-04-26T05:07:30.663694Z","shell.execute_reply.started":"2022-04-26T05:07:30.657491Z","shell.execute_reply":"2022-04-26T05:07:30.662793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_sim_matrix(s1, s2):\n    \n    #Get token-wise embedding\n    #s1_token_emb = np.array([get_word_embedding(word, word_model) for word in s1])\n    #s1_token_emb = np.array([word.to('cpu') for word in get_word_embedding(s1)])\n    #s1_token_emb = np.array([word.detach().to('cpu').numpy() for word in get_word_embedding(s1)])\n    #print(type(s1_token_emb))#x.detach().cpu().numpy()\n    #Normalize\n    s1_token_emb = s1\n    s1_token_emb/=np.linalg.norm(s1_token_emb, axis = 1, ord = 2, keepdims=True)\n    \n    #Get token-wise embedding of icd text\n    #s2_token_emb = np.array([get_word_embedding(word, word_model) for word in s2])\n    #s2_token_emb = np.array([word.to('cpu') for word in get_word_embedding(s2)])\n    #s2_token_emb = np.array([word.detach().to('cpu').numpy()for word in get_word_embedding(s2)])\n    #Normalize\n    #print(type(s2_token_emb))\n    s2_token_emb = s2\n    s2_token_emb/=np.linalg.norm(s2_token_emb, axis = 1, ord = 2, keepdims=True)\n    \n    \n    sim_matrix = s1_token_emb.dot(s2_token_emb.T)\n    #print(\"Here is the sim matrix \",(sim_matrix))\n    \n    return np.clip(sim_matrix, -1.0, 1.0) #Clip the Elements of the matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:31.297451Z","iopub.execute_input":"2022-04-26T05:07:31.297705Z","iopub.status.idle":"2022-04-26T05:07:31.303428Z","shell.execute_reply.started":"2022-04-26T05:07:31.297676Z","shell.execute_reply":"2022-04-26T05:07:31.302736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_text_vector, freq_test = get_tf_modified(df_icd_cleaned['Long Description'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:32.135023Z","iopub.execute_input":"2022-04-26T05:07:32.135554Z","iopub.status.idle":"2022-04-26T05:07:33.221527Z","shell.execute_reply.started":"2022-04-26T05:07:32.135516Z","shell.execute_reply":"2022-04-26T05:07:33.220786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#freq_test","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:34.994332Z","iopub.execute_input":"2022-04-26T05:07:34.995069Z","iopub.status.idle":"2022-04-26T05:07:34.999598Z","shell.execute_reply.started":"2022-04-26T05:07:34.995029Z","shell.execute_reply":"2022-04-26T05:07:34.998579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(get_tf_vector(\"acute kidney failure acute \", freq_text_vector))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:36.013632Z","iopub.execute_input":"2022-04-26T05:07:36.014259Z","iopub.status.idle":"2022-04-26T05:07:36.023762Z","shell.execute_reply.started":"2022-04-26T05:07:36.014219Z","shell.execute_reply":"2022-04-26T05:07:36.023092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(get_tf_vector_modified(\"acute kidney failure acute \", freq_text_vector))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:37.530031Z","iopub.execute_input":"2022-04-26T05:07:37.530825Z","iopub.status.idle":"2022-04-26T05:07:37.538877Z","shell.execute_reply.started":"2022-04-26T05:07:37.530786Z","shell.execute_reply":"2022-04-26T05:07:37.538116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icd_sentences_embed_test = [[word.detach().to('cpu').numpy()for word in get_word_embedding(icd_text)] for icd_text in icd_sentences]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:07:38.861454Z","iopub.execute_input":"2022-04-26T05:07:38.862016Z","iopub.status.idle":"2022-04-26T05:08:24.760918Z","shell.execute_reply.started":"2022-04-26T05:07:38.861973Z","shell.execute_reply":"2022-04-26T05:08:24.76015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dict(freq_test)['Frequency']","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.762786Z","iopub.execute_input":"2022-04-26T05:08:24.763111Z","iopub.status.idle":"2022-04-26T05:08:24.766853Z","shell.execute_reply.started":"2022-04-26T05:08:24.763074Z","shell.execute_reply":"2022-04-26T05:08:24.766144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#(freq_test.loc['unspecified', :]).values","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.768044Z","iopub.execute_input":"2022-04-26T05:08:24.768907Z","iopub.status.idle":"2022-04-26T05:08:24.775549Z","shell.execute_reply.started":"2022-04-26T05:08:24.768871Z","shell.execute_reply":"2022-04-26T05:08:24.774911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#((icd_sentences_embed_test)[0])[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.777716Z","iopub.execute_input":"2022-04-26T05:08:24.777997Z","iopub.status.idle":"2022-04-26T05:08:24.783397Z","shell.execute_reply.started":"2022-04-26T05:08:24.777962Z","shell.execute_reply":"2022-04-26T05:08:24.782691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reset Debugging Counter\nk = 0\n#Calculate the Similarity Score between 2 senteces\ndef RWMS_similarity_old(sent_1, sent_2, freq_vec,icd_text_embed):\n    global k\n    try:\n        \n        #get tf vectors\n        #start = time.time()\n        sent1_tf_vec, sent1_list = get_tf_vector(sent_1, freq_vec)\n        #sent2_tf_vec, sent2 = get_tf_vector(sent_2, freq_vec)\n        \n        #tokenized sentences\n        #sent1 = sent1#list(set(word_tokenize(sent_1)))\n       # sent2 = list(set(word_tokenize(sent_2)))\n\n        #Compute Cosine Similarity Matrix\n        sim_mat = cosine_sim_matrix(icd_text_embed, sent_2)\n        #print('Sim_mat:\\n', sim_mat)\n        \n        #Initialise Similarity values:\n        W_s_t = 0\n        #W_t_s = 0\n\n        #for S->t (icd_text -> dd text)\n        max_cosine_vec = sim_mat.max(axis = 1)\n        #print('Sim_mat:\\n', sim_mat)\n        \n        # Take Elementwise product of max cos similarity vector and tf vector\n        sim_vec = sent1_tf_vec.to_numpy().transpose() * max_cosine_vec\n        \n        #Take sum of all elements of sim_vec to obtain final similarity\n        W_s_t = np.sum(sim_vec)\n        \n        del sim_mat, sent1_tf_vec#sent1, sent2\n        \n        #for t->S\n        #for j in range(len(sent2)):\n        #    w2 = sent2[j]\n        #    sim = 0\n        #    if w2 in density_tfidf.index:\n        #        #index of word with max cosine similarity from sent1\n        #        max_j = np.argmax(sim_mat.transpose()[j])\n\n        #        #multiply the tfidf value of current word of sent2 with cosine similarity of the most similar from sent1\n        #        sim = sim_mat.transpose()[j][max_j]*density_tfidf['s2_tfidf'][w2]\n        #    else:\n        #        sim = 0\n        #    W_t_s+=sim\n        \n        \n        #k = k + 1\n        #print('k: ', k)\n\n        return W_s_t#, sim_mat, max_cosine_vec, sent1_tf_vec, sent1, sent2   #min(W_s_t, W_t_s)\n    except:\n        #print('sent_1: ', sent_1)\n        sent_1_emb = get_sentence_embedding(icd_text_embed)\n        #print('sent_2: ', sent_2)\n        sent_2_emb = get_sentence_embedding(sent_2)\n        \n        #k = k + 1\n        #print('k: ', k)\n        \n        return sent_1_emb.dot(sent_2_emb.T)#, 0, 0, 0, 0, 0#cosine_sim(sent_1_emb, sent_2_emb)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.784614Z","iopub.execute_input":"2022-04-26T05:08:24.785121Z","iopub.status.idle":"2022-04-26T05:08:24.794489Z","shell.execute_reply.started":"2022-04-26T05:08:24.785085Z","shell.execute_reply":"2022-04-26T05:08:24.793838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reset Debugging Counter\nk = 0\n#Calculate the Similarity Score between 2 senteces\ndef RWMS_similarity(sent_1, sent_2, freq_vec,icd_text_embed):\n    global k\n    try:\n        \n        #get tf vectors\n        #start = time.time()\n        sent1_tf_vec, sent1_list = get_tf_vector_modified(sent_1, freq_vec)\n        #sent2_tf_vec, sent2 = get_tf_vector(sent_2, freq_vec)\n        \n        #tokenized sentences\n        #sent1 = sent1#list(set(word_tokenize(sent_1)))\n       # sent2 = list(set(word_tokenize(sent_2)))\n\n        #Compute Cosine Similarity Matrix\n        sim_mat = cosine_sim_matrix(icd_text_embed[:len(sent1_list)], sent_2)\n        #print('Sim_mat:\\n', sim_mat)\n        \n        #Initialise Similarity values:\n        W_s_t = 0\n        #W_t_s = 0\n\n        #for S->t (icd_text -> dd text)\n        max_cosine_vec = sim_mat.max(axis = 1)\n        #print('Sim_mat:\\n', sim_mat)\n        \n        # Take Elementwise product of max cos similarity vector and tf vector\n        sim_vec = sent1_tf_vec.to_numpy().transpose() * max_cosine_vec\n        \n        #Take sum of all elements of sim_vec to obtain final similarity\n        W_s_t = np.sum(sim_vec)\n        \n        del sim_mat, sent1_tf_vec#sent1, sent2\n        \n        #for t->S\n        #for j in range(len(sent2)):\n        #    w2 = sent2[j]\n        #    sim = 0\n        #    if w2 in density_tfidf.index:\n        #        #index of word with max cosine similarity from sent1\n        #        max_j = np.argmax(sim_mat.transpose()[j])\n\n        #        #multiply the tfidf value of current word of sent2 with cosine similarity of the most similar from sent1\n        #        sim = sim_mat.transpose()[j][max_j]*density_tfidf['s2_tfidf'][w2]\n        #    else:\n        #        sim = 0\n        #    W_t_s+=sim\n        \n        \n        #k = k + 1\n        #print('k: ', k)\n\n        return W_s_t#, sim_mat, max_cosine_vec, sent1_tf_vec, sent1, sent2   #min(W_s_t, W_t_s)\n    except:\n        #print('sent_1: ', sent_1)\n        sent_1_emb = get_sentence_embedding(icd_text_embed)\n        #print('sent_2: ', sent_2)\n        sent_2_emb = get_sentence_embedding(sent_2)\n        \n        #k = k + 1\n        #print('k: ', k)\n        \n        return sent_1_emb.dot(sent_2_emb.T)#, 0, 0, 0, 0, 0#cosine_sim(sent_1_emb, sent_2_emb)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.795915Z","iopub.execute_input":"2022-04-26T05:08:24.796188Z","iopub.status.idle":"2022-04-26T05:08:24.807207Z","shell.execute_reply.started":"2022-04-26T05:08:24.796136Z","shell.execute_reply":"2022-04-26T05:08:24.806569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert ICD Indices to hot vectors of length: NUM_ICD_CODE\ndef generate_binary(icd_index_list, total_icd = NUM_ICD_CODES):\n    binary_icd = np.zeros(total_icd)\n    for x in icd_index_list:\n        binary_icd[x] = 1\n        \n    return binary_icd","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.808293Z","iopub.execute_input":"2022-04-26T05:08:24.808653Z","iopub.status.idle":"2022-04-26T05:08:24.816676Z","shell.execute_reply.started":"2022-04-26T05:08:24.808618Z","shell.execute_reply":"2022-04-26T05:08:24.815948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate Specificity\n\ndef calc_specificity(pred_vec, act_vec):\n    m = len(pred_vec)\n    \n    specificity = []\n    \n    for i in range(m):\n        pred, act = pred_vec[i], act_vec[i]\n        dict_XY = {'pred':pred, 'act':act}\n        res = pd.DataFrame(dict_XY, columns=['pred', 'act'])\n        res['Sum'] = res['pred'] + res['act']\n        \n        pred_p = res[res.pred == 1]\n        pred_n = res[res.act == 1]\n        \n        tp = len(res[res.Sum == 2])\n        tn = len(res[res.Sum == 0]) \n        \n        fp = len(pred_p[pred_p.act == 0])\n        fn = len(pred_n[pred_n.act == 1])\n        \n        #Calculate specificity for this example:\n        spec = tn/(tn + fp)\n        \n        specificity.append(spec)\n    \n    return sum(specificity) / float(m)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.817842Z","iopub.execute_input":"2022-04-26T05:08:24.818176Z","iopub.status.idle":"2022-04-26T05:08:24.827434Z","shell.execute_reply.started":"2022-04-26T05:08:24.818141Z","shell.execute_reply":"2022-04-26T05:08:24.82662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generateTensorsOnCodeTextData(text):\n    #text = text.split()\n    sentenceSize  = 125\n    line = convert_to_unicode(text)\n    line  = cleanText(text)\n    tokens = tokenizer.tokenize(line)\n    sentLength  = len(tokens)\n    #print(\"\\ntotal token count \", len(tokens))\n    padTokens  = []\n    all_tokens = [tokens[i: i+sentenceSize] for i in range(0, len(tokens), sentenceSize)]\n    if(len(all_tokens) == 1):\n        padTokens.append(\" \".join(all_tokens[0]))\n        padTokens.append(\" \".join([\"<pad>\" for _ in range(sentenceSize)]))\n        #return padTokens\n    else:\n        if(not (len(all_tokens)% 2)):\n            padTokens = [\" \".join(tok) for tok in all_tokens]\n            #return padTokens\n        else:\n            padTokens = [\" \".join(tok) for tok in all_tokens]\n            padTokens.append(\" \".join([\"<pad>\" for _ in range(sentenceSize)]))\n            #return padTokens\n    text = [padTokens[i: i+2] for i in range(0, len(padTokens), 2)]\n    bert_input_ = []\n    bert_label_ = []\n    segment_label_ = []\n    is_next_ = []\n    for i in range(len(text)):\n            t1, t2, is_next_label = text[i][0], text[i][1], 1#(i, text)#generate the label for next sentence prediction in the form of 0 or 1\n            t1 = [newvocab.stoi.get(tko, newvocab.unk_index) for tko in t1.split()]\n            t2 = [newvocab.stoi.get(tko, newvocab.unk_index) for tko in t2.split()]\n\n        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n            t1 = [newvocab.sos_index] + t1 + [newvocab.eos_index]\n            t2 = t2 + [newvocab.eos_index]\n            segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:256]\n            bert_input = (t1 + t2)[:256]\n            padding = [newvocab.pad_index for _ in range(256 - len(bert_input))]#This would pad the input, output lable and sengement label up to the maximum length\n            bert_input.extend(padding), segment_label.extend(padding)\n            bert_input_.append(bert_input)\n            segment_label_.append(segment_label)\n    return torch.tensor(bert_input_), torch.tensor(segment_label_)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.828695Z","iopub.execute_input":"2022-04-26T05:08:24.829254Z","iopub.status.idle":"2022-04-26T05:08:24.843475Z","shell.execute_reply.started":"2022-04-26T05:08:24.829218Z","shell.execute_reply":"2022-04-26T05:08:24.84284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict ICD 9 Codes\n\n#construct the Term Frequency Vector using just the icd corpus\nfreq_vec, freq_vec_dict = get_tf_modified(df_icd_cleaned['Long Description'])\n\nstart_time = time.time()\n\n#Calculate the Similarity of each document with each icd text\nX = DD_Data['clean_text']#[0]\nY = list(DD_Data['ICD9_CODE'])\nm = len(Y)\n\nsim = []\nicd_sentences_embed = [[word.detach().to('cpu').numpy()for word in get_word_embedding(icd_text)] for icd_text in icd_sentences]\nprint(\"\\nEmbeddings saved for code\")\n\nfor i in range(m):\n    print(\"processing doc \",i)\n    text = X[i]\n    text_embed = [word.detach().to('cpu').numpy()for word in get_word_embedding(text)]\n    #print(type(text_embed), text_embed)\n    #Modified#Calculate cosine similarity of sentence with icd sentences\n    sim.append(np.array([RWMS_similarity(icd_sentences[j], text_embed, freq_vec,icd_text_embed) for j, icd_text_embed in enumerate(icd_sentences_embed)]))#cosine_sim1_matrix(sent_emb)\n    \n    #reset debugging counter\n    #global k\n    #k = 0\n    \n    if (i+1)%10 == 0:\n        print(str(i+1) + '/' + str(m) + ' records done.')\n    \n    if (i+1)%100 == 0:\n        print('\\nTime elapsed: ', (time.time() - start_time)/60.0, ' minutes')\n\n    if (i+1)%1000 == 0:\n        sim_df = pd.DataFrame(sim, dtype = float)\n        sim_df.to_csv('Similarity_Data/Optimized_NP_RWMS_Negation_similarity_data_' + str(i+1) + '_records.csv')\n        #sim_df.to_csv('Optimized_NP_RWMS_similarity_data_' + (i+1) + '_records.csv')\n\ntime_elapsed = time.time() - start_time\nprint('\\nTime elapsed: ', time_elapsed/60.0, ' minutes')\n\n#Save the similarity scores in an excel file\nprint('Saving Similarity scores in file......')\nsim_df = pd.DataFrame(sim, dtype = float)\nsim_df.to_csv('Similarity_Data_Optimized_NP_RWMS_Negation_similarity_data_' + str(m) + '_records.csv')\n#sim_df.to_csv('Optimized_NP_RWMS_similarity_data_' + str(m) + '_records.csv')\nprint('Similarity Scores Saved!')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:08:24.846012Z","iopub.execute_input":"2022-04-26T05:08:24.846427Z","iopub.status.idle":"2022-04-26T05:19:12.348854Z","shell.execute_reply.started":"2022-04-26T05:08:24.846392Z","shell.execute_reply":"2022-04-26T05:19:12.34802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load Similarity Data\nsim = pd.read_csv('/kaggle/working/Similarity_Data_Optimized_NP_RWMS_Negation_similarity_data_'+str(m)+'_records.csv')\n#sim = pd.read_csv('Optimized_NP_RWMS_similarity_data_' + str(m) + '_records.csv')\n\n#Drop first column\nsim = sim.drop(['Unnamed: 0'], axis = 1)\n\n#Convert DataFrame to List\nsim = np.array(sim.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:19:12.350314Z","iopub.execute_input":"2022-04-26T05:19:12.350756Z","iopub.status.idle":"2022-04-26T05:19:12.50819Z","shell.execute_reply.started":"2022-04-26T05:19:12.350706Z","shell.execute_reply":"2022-04-26T05:19:12.507448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_vec","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:19:12.509408Z","iopub.execute_input":"2022-04-26T05:19:12.509654Z","iopub.status.idle":"2022-04-26T05:19:12.519583Z","shell.execute_reply.started":"2022-04-26T05:19:12.509624Z","shell.execute_reply":"2022-04-26T05:19:12.518784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save Predictions and Corresponding Scores per threshold\n\n#Initialise Similarity Result Dataframe\nresults_sim = pd.DataFrame(columns = ['Actual Code', \n                                  'Predicted Codes',\n                                  'Similarity Codes'])\n\n\nthreshold_list = [0.3,0.4,0.5,0.6, 0.7, 0.8, 0.9]\n\n#Initialize Excel\n#writer = pd.ExcelWriter('Similarity_Data/Code and Similarity Data - Unsupervised ICD - ' + str(m) + ' Records.xlsx', engine='xlsxwriter')\n\nfor thresh in threshold_list:\n\n    #Print Threshold:\n    print('\\nCommenced for Similarity Threshold:', thresh)\n\n    #X = DD_Data['clean_text'][0]\n    #Y = list(DD_Data['ICD9_CODE'])\n\n    data_input_format = []\n\n    i = 0\n    for i in range(m):\n        #Convert Actual Codes to indices. Consider only those codes present in icd_codes\n        act_codes = [str(y) for y in Y[i][2:-2].split(\"', '\") if y in icd_codes]#To get rid of '' and ,\n        act_codes_string = ', '.join(act_codes)\n\n        #Apply thresholding to obtain indices of similar embeddings\n        #print(sim)\n        threshold_filter = sim[i] > thresh\n\n\n        #Similarity >= threshold --> similarity \n        #Similarity < threshold --> 0\n        sim_post_threshold = sim[i]*threshold_filter\n\n        #Add indices of non zero elements to pred\n        pred_indices = np.nonzero(sim_post_threshold)[0].tolist()\n\n        #Get codes from indices\n        pred_codes = [str(dict_index_code[p]) for p in pred_indices]\n        #Convert to Comma Separated String\n        pred_codes_string = ', '.join(pred_codes)\n        #print([pred_codes_string])\n\n        #Corresponding Similarity Scores of List of Indices\n        sim_scores = [str(sim[i][j]) for j in pred_indices]\n        #Convert to Comma Separated String\n        sim_scores_string = ', '.join(sim_scores)\n        #print([sim_scores_string])\n\n\n        data_input = [act_codes_string, pred_codes_string, sim_scores_string]\n\n        #Append\n        data_input_format.append(data_input)\n\n    df = pd.DataFrame(data_input_format, columns = ['Actual Codes', 'Predicted Codes', 'Similarity Scores'])\n    df.to_csv('Similarity_Data_Negation - Code and Similarity Data - Unsupervised ICD - ' + str(m) + ' Records_' + str(thresh) + '_threshold.csv')\n    #df.to_csv('NP - Predicted Codes and Similarity Data - Unsupervised ICD - ' + str(m) + ' Records_' + str(thresh) + '_threshold.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:19:45.450788Z","iopub.execute_input":"2022-04-26T05:19:45.451459Z","iopub.status.idle":"2022-04-26T05:19:46.375754Z","shell.execute_reply.started":"2022-04-26T05:19:45.451416Z","shell.execute_reply":"2022-04-26T05:19:46.375008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl\nimport openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:19:47.854039Z","iopub.execute_input":"2022-04-26T05:19:47.854769Z","iopub.status.idle":"2022-04-26T05:19:55.730306Z","shell.execute_reply.started":"2022-04-26T05:19:47.854708Z","shell.execute_reply":"2022-04-26T05:19:55.729492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate Metrics\n\n#Initialize Results Dataframe which will store the relevant metrics for each threshold\nresults = pd.DataFrame(columns = ['Number of Observations', \n                                  'Number of ICD Codes',\n                                  'Threshold',\n                                  'Precision', \n                                  'Recall', \n                                  'F1-Score', \n                                  'Specificity'])\n\n#Compute Accuracy metrics based on thresholding        \n#threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n#threshold_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nthreshold_list = [0.3,0.4,0.45,0.5 ,0.52,0.55,0.6, 0.7, 0.8, 0.9]\n#threshold_list = [0.4]\n#threshold_list = [1.0, 5.0 10.0, 15.0, 20.0]\n\nstart_time = time.time()\n\nfor thresh in threshold_list:\n\n    #Print Threshold:\n    print('\\nCommenced for Similarity Threshold:', thresh)\n    \n    #X = DD_Data['clean_text'][0]\n    #Y = list(DD_Data['ICD9_CODE'])\n    \n    predicted_codes_index = []\n    actual_codes_index = []\n\n    #m = 1#len(Y)\n\n    for i in range(m):\n        #text = X#[i]\n        #print(i)\n        #Convert Actual Codes to indices. Consider only those codes present in icd_codes\n        act_icd_indices = [dict_code_index[y] for y in Y[i][2:-2].split(\"', '\") if y in icd_codes] #To get rid of '' and ,\n\n        #Initialize Prediction list. This will contain indices of similar icd codes\n        pred = []\n\n        #Apply thresholding to obtain indices of similar embeddings\n        #print(sim)\n        threshold_filter = sim[i] > thresh\n\n        \n        #Similarity >= threshold --> similarity \n        #Similarity < threshold --> 0\n        sim_post_threshold = sim[i]*threshold_filter\n\n        #Add indices of non zero elements to pred\n        pred.extend(np.nonzero(sim_post_threshold)[0].tolist())\n\n        #Consider the unique indices\n        pred = set(pred)\n        #print(pred,\"\\n\",act_icd_indices)\n        #Convert list of indices to hot vector\n        pred_icd_vector = generate_binary(pred)\n        act_icd_vector = generate_binary(act_icd_indices)\n\n        #Append\n        #print(act_icd_vector,\"\\n\",pred_icd_vector)\n        predicted_codes_index.append(pred_icd_vector)\n        actual_codes_index.append(act_icd_vector)\n\n\n        #if i%100 == 0 and i != 0:\n        #    print(i, ' sentences done.')\n\n    #Calculate Precision, Recall, F1 Score for this threshold\n    precision, recall, f1_score, support = precision_recall_fscore_support(actual_codes_index, predicted_codes_index, average = 'micro')\n    specificity = calc_specificity(predicted_codes_index, actual_codes_index)\n\n    #Add to results Dataframe:\n    results = results.append({'Number of Observations': m, \n                    'Number of ICD Codes': NUM_ICD_CODES, \n                    'Threshold': thresh,\n                    'Precision': precision, \n                    'Recall': recall, \n                    'F1-Score': f1_score, \n                    'Specificity': specificity}, ignore_index=True)\n\n    print('\\nPrecision: ', precision, ' Recall: ', recall, ' F1-Score: ', f1_score, ' Specificity:', specificity)\n\n\ntime_elapsed = time.time() - start_time\nprint('\\nTime elapsed: ', time_elapsed/60.0, ' minutes')    \n    \n#Print Results\nRESULT_FILENAME = 'Results_Experiment_Results_RWMS_Negation_Documentwise_' + str(DD_Data.shape[0]) + '_records_' + str(NUM_ICD_CODES) + '_codes_' + '.xlsx'\n#RESULT_FILENAME = 'Experiment_Results_NP_RWMS_Documentwise_' + str(DD_Data.shape[0]) + '_records_' + str(NUM_ICD_CODES) + '_codes_' + '.xlsx'\nresults.to_excel(RESULT_FILENAME) \nprint('Results Saved!')\nresults","metadata":{"execution":{"iopub.status.busy":"2022-04-26T05:25:10.396292Z","iopub.execute_input":"2022-04-26T05:25:10.396587Z","iopub.status.idle":"2022-04-26T05:25:12.71949Z","shell.execute_reply.started":"2022-04-26T05:25:10.396557Z","shell.execute_reply":"2022-04-26T05:25:12.71878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}