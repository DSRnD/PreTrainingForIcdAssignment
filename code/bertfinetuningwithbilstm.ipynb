{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.129255,"end_time":"2022-02-14T21:23:12.636081","exception":false,"start_time":"2022-02-14T21:23:12.506826","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:04.634270Z","iopub.execute_input":"2022-05-02T15:29:04.634590Z","iopub.status.idle":"2022-05-02T15:29:04.712711Z","shell.execute_reply.started":"2022-05-02T15:29:04.634507Z","shell.execute_reply":"2022-05-02T15:29:04.712009Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data =  pd.read_csv('/kaggle/input/mimic-clean-text/MIMIC-III-Final_cleaned.csv')[:15000]","metadata":{"papermill":{"duration":16.190449,"end_time":"2022-02-14T21:23:31.794862","exception":false,"start_time":"2022-02-14T21:23:15.604413","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:06.425361Z","iopub.execute_input":"2022-05-02T15:29:06.425631Z","iopub.status.idle":"2022-05-02T15:29:21.636140Z","shell.execute_reply.started":"2022-05-02T15:29:06.425602Z","shell.execute_reply":"2022-05-02T15:29:21.635408Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.shape, data.columns","metadata":{"papermill":{"duration":0.103121,"end_time":"2022-02-14T21:23:31.986297","exception":false,"start_time":"2022-02-14T21:23:31.883176","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:21.637949Z","iopub.execute_input":"2022-05-02T15:29:21.638218Z","iopub.status.idle":"2022-05-02T15:29:21.646578Z","shell.execute_reply.started":"2022-05-02T15:29:21.638181Z","shell.execute_reply":"2022-05-02T15:29:21.645923Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#data['length'] = data.apply(lambda x:len(x['clean_text'].split()) , axis=1)","metadata":{"papermill":{"duration":0.095793,"end_time":"2022-02-14T21:23:32.17092","exception":false,"start_time":"2022-02-14T21:23:32.075127","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:21.647800Z","iopub.execute_input":"2022-05-02T15:29:21.649514Z","iopub.status.idle":"2022-05-02T15:29:21.654756Z","shell.execute_reply.started":"2022-05-02T15:29:21.649473Z","shell.execute_reply":"2022-05-02T15:29:21.653993Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"papermill":{"duration":0.11472,"end_time":"2022-02-14T21:23:32.373139","exception":false,"start_time":"2022-02-14T21:23:32.258419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:21.658518Z","iopub.execute_input":"2022-05-02T15:29:21.658739Z","iopub.status.idle":"2022-05-02T15:29:21.678478Z","shell.execute_reply.started":"2022-05-02T15:29:21.658715Z","shell.execute_reply":"2022-05-02T15:29:21.677613Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Create Processed Data for training the bert pytorch model  ","metadata":{"papermill":{"duration":0.090811,"end_time":"2022-02-14T21:23:32.553459","exception":false,"start_time":"2022-02-14T21:23:32.462648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf","metadata":{"papermill":{"duration":5.561268,"end_time":"2022-02-14T21:23:38.205084","exception":false,"start_time":"2022-02-14T21:23:32.643816","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:21.679849Z","iopub.execute_input":"2022-05-02T15:29:21.680170Z","iopub.status.idle":"2022-05-02T15:29:25.784286Z","shell.execute_reply.started":"2022-05-02T15:29:21.680132Z","shell.execute_reply":"2022-05-02T15:29:25.783536Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorflow==1.14\n#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"papermill":{"duration":0.097929,"end_time":"2022-02-14T21:23:38.393928","exception":false,"start_time":"2022-02-14T21:23:38.295999","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.785444Z","iopub.execute_input":"2022-05-02T15:29:25.785694Z","iopub.status.idle":"2022-05-02T15:29:25.788749Z","shell.execute_reply.started":"2022-05-02T15:29:25.785661Z","shell.execute_reply":"2022-05-02T15:29:25.788098Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")","metadata":{"papermill":{"duration":0.099491,"end_time":"2022-02-14T21:23:38.581351","exception":false,"start_time":"2022-02-14T21:23:38.48186","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.790241Z","iopub.execute_input":"2022-05-02T15:29:25.790636Z","iopub.status.idle":"2022-05-02T15:29:25.800801Z","shell.execute_reply.started":"2022-05-02T15:29:25.790596Z","shell.execute_reply":"2022-05-02T15:29:25.800168Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport six\nimport tensorflow as tf","metadata":{"papermill":{"duration":0.095099,"end_time":"2022-02-14T21:23:38.764331","exception":false,"start_time":"2022-02-14T21:23:38.669232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.802171Z","iopub.execute_input":"2022-05-02T15:29:25.802599Z","iopub.status.idle":"2022-05-02T15:29:25.812805Z","shell.execute_reply.started":"2022-05-02T15:29:25.802552Z","shell.execute_reply":"2022-05-02T15:29:25.812012Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class BasicTokenizer(object):\n  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n  def __init__(self, do_lower_case=True):\n    \"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    \"\"\"Tokenizes a piece of text.\"\"\"\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)#perform invalid char removal and whitespace removal\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    \"\"\"Strips accents from a piece of text.\"\"\"\n    text = unicodedata.normalize(\"NFD\", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == \"Mn\":\n        continue\n      output.append(char)\n    return \"\".join(output)\n\n  def _run_split_on_punc(self, text):\n    \"\"\"Splits punctuation on a piece of text.\"\"\"\n    chars = list(text)\n    i = 0\n    start_new_word = True \n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return [\"\".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    \"\"\"Adds whitespace around any CJK character.\"\"\"\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append(\" \")\n        output.append(char)\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)\n\n  def _is_chinese_char(self, cp):\n    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)","metadata":{"papermill":{"duration":0.113927,"end_time":"2022-02-14T21:23:38.967518","exception":false,"start_time":"2022-02-14T21:23:38.853591","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.815797Z","iopub.execute_input":"2022-05-02T15:29:25.816329Z","iopub.status.idle":"2022-05-02T15:29:25.984417Z","shell.execute_reply.started":"2022-05-02T15:29:25.816297Z","shell.execute_reply":"2022-05-02T15:29:25.983636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def load_vocab(vocab_file):\n  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n  vocab = collections.OrderedDict()\n  index = 0\n  #with tf.gfile.GFile(vocab_file, \"r\") as reader:\n  with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = (token.strip()).split(\"\\t\")\n      vocab[token[0]] = index\n      index += 1\n  print(\"\\nVocab loading done!\")\n  return vocab","metadata":{"papermill":{"duration":0.099011,"end_time":"2022-02-14T21:23:39.155525","exception":false,"start_time":"2022-02-14T21:23:39.056514","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.989503Z","iopub.execute_input":"2022-05-02T15:29:25.989723Z","iopub.status.idle":"2022-05-02T15:29:25.998154Z","shell.execute_reply.started":"2022-05-02T15:29:25.989691Z","shell.execute_reply":"2022-05-02T15:29:25.997346Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class WordpieceTokenizer(object):\n  \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    \"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        #print(\"\\nunknown token inserted\")\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = \"\".join(chars[start:end])\n          if start > 0:\n            substr = \"##\" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            #print(\"\\nknown token inserted\", cur_substr)\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n        #print(\"\\nunknown token inserted\") #This is where unknown tokens are being inserted\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return True\n  cat = unicodedata.category(char)\n  if cat == \"Zs\":\n    return True\n  return False\n\n\ndef _is_control(char):\n  \"\"\"Checks whether `chars` is a control character.\"\"\"\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (\"Cc\", \"Cf\"):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(\"P\"):\n    return True\n  return False","metadata":{"papermill":{"duration":0.109211,"end_time":"2022-02-14T21:23:39.354243","exception":false,"start_time":"2022-02-14T21:23:39.245032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:25.999617Z","iopub.execute_input":"2022-05-02T15:29:26.002253Z","iopub.status.idle":"2022-05-02T15:29:26.017953Z","shell.execute_reply.started":"2022-05-02T15:29:26.002211Z","shell.execute_reply":"2022-05-02T15:29:26.017202Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class FullTokenizer(object):\n  \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)#Create Vocab dist with index 0\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}#Get vocab dict with index as key and token as value\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)#Initialize the basic tokenizer \n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      #for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)","metadata":{"papermill":{"duration":0.101425,"end_time":"2022-02-14T21:23:39.543831","exception":false,"start_time":"2022-02-14T21:23:39.442406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.019446Z","iopub.execute_input":"2022-05-02T15:29:26.020238Z","iopub.status.idle":"2022-05-02T15:29:26.031194Z","shell.execute_reply.started":"2022-05-02T15:29:26.020196Z","shell.execute_reply":"2022-05-02T15:29:26.030519Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenizer = FullTokenizer(\n      vocab_file='/kaggle/input/autoencoderdecoderonpmcembeddingvocab/Vocab.txt', do_lower_case=True)","metadata":{"papermill":{"duration":0.298218,"end_time":"2022-02-14T21:23:39.929494","exception":false,"start_time":"2022-02-14T21:23:39.631276","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.034417Z","iopub.execute_input":"2022-05-02T15:29:26.034630Z","iopub.status.idle":"2022-05-02T15:29:26.211492Z","shell.execute_reply.started":"2022-05-02T15:29:26.034604Z","shell.execute_reply":"2022-05-02T15:29:26.210661Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def convert_by_vocab(vocab, items):\n  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens","metadata":{"papermill":{"duration":0.101803,"end_time":"2022-02-14T21:23:40.145931","exception":false,"start_time":"2022-02-14T21:23:40.044128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.212883Z","iopub.execute_input":"2022-05-02T15:29:26.213323Z","iopub.status.idle":"2022-05-02T15:29:26.220380Z","shell.execute_reply.started":"2022-05-02T15:29:26.213282Z","shell.execute_reply":"2022-05-02T15:29:26.219490Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer.tokenize(\"\\This is the string\")","metadata":{"papermill":{"duration":0.103389,"end_time":"2022-02-14T21:23:40.339196","exception":false,"start_time":"2022-02-14T21:23:40.235807","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.223458Z","iopub.execute_input":"2022-05-02T15:29:26.224084Z","iopub.status.idle":"2022-05-02T15:29:26.233345Z","shell.execute_reply.started":"2022-05-02T15:29:26.224019Z","shell.execute_reply":"2022-05-02T15:29:26.232520Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def cleanText(sentence):\n        sentence = re.sub(\n        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n        sentence = re.sub(r\"\\,+\", \",\", sentence)\n        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n        sentence = re.sub(r'[0-9]', \" \", sentence)\n        sentence = re.sub(r'[^\\w\\s]','',sentence)\n        sentence = re.sub(r'(?:^| )\\w(?:$| )', ' ', (sentence)).strip()#AKS\n        sentence = sentence.lower()\n        #return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]\n        return sentence","metadata":{"papermill":{"duration":0.102303,"end_time":"2022-02-14T21:23:40.53316","exception":false,"start_time":"2022-02-14T21:23:40.430857","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.234671Z","iopub.execute_input":"2022-05-02T15:29:26.235269Z","iopub.status.idle":"2022-05-02T15:29:26.243470Z","shell.execute_reply.started":"2022-05-02T15:29:26.235099Z","shell.execute_reply":"2022-05-02T15:29:26.242465Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def processTextIntoSentences(text):\n    if(not len(text)):\n        print(\"\\nGiven string is empty. Please check again\")\n    sentenceSize  = 125\n    #sentenceSize = 256\n    line = convert_to_unicode(text)\n    line  = cleanText(text)\n    tokens = tokenizer.tokenize(line)\n    #print(\"\\nTokens\", tokens)\n    if(tokens):\n        all_tokens = [tokens[i: i+sentenceSize] for i in range(0, len(tokens), sentenceSize)]\n        #print(\"\\nbatched tokens\", len(all_tokens), all_tokens)\n        processedTokens = []\n        for token_ in all_tokens:\n            processedTokens.append(\" \".join(token_))\n        #print(len(processedTokens[1].split()))\n        return (\"|\".join(processedTokens))\n    \n            \n        ","metadata":{"papermill":{"duration":0.100782,"end_time":"2022-02-14T21:23:40.724374","exception":false,"start_time":"2022-02-14T21:23:40.623592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.244785Z","iopub.execute_input":"2022-05-02T15:29:26.245571Z","iopub.status.idle":"2022-05-02T15:29:26.254739Z","shell.execute_reply.started":"2022-05-02T15:29:26.245540Z","shell.execute_reply":"2022-05-02T15:29:26.253959Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#processTextIntoSentences(a)","metadata":{"papermill":{"duration":0.097986,"end_time":"2022-02-14T21:23:40.913939","exception":false,"start_time":"2022-02-14T21:23:40.815953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.257298Z","iopub.execute_input":"2022-05-02T15:29:26.257856Z","iopub.status.idle":"2022-05-02T15:29:26.263386Z","shell.execute_reply.started":"2022-05-02T15:29:26.257818Z","shell.execute_reply":"2022-05-02T15:29:26.262563Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data['processedText'] = data.apply(lambda x:processTextIntoSentences(x['clean_text']) , axis=1)","metadata":{"papermill":{"duration":430.186712,"end_time":"2022-02-14T21:30:51.192729","exception":false,"start_time":"2022-02-14T21:23:41.006017","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T15:29:26.265115Z","iopub.execute_input":"2022-05-02T15:29:26.265545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape, data.columns, len((data['processedText'][0]).split('|')[0].split())","metadata":{"papermill":{"duration":0.101755,"end_time":"2022-02-14T21:30:51.385115","exception":false,"start_time":"2022-02-14T21:30:51.28336","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the vocabulary on training dataset","metadata":{"papermill":{"duration":0.089988,"end_time":"2022-02-14T21:30:51.564883","exception":false,"start_time":"2022-02-14T21:30:51.474895","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.090389,"end_time":"2022-02-14T21:30:51.74633","exception":false,"start_time":"2022-02-14T21:30:51.655941","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pickle\nimport tqdm\nfrom collections import Counter","metadata":{"papermill":{"duration":0.098474,"end_time":"2022-02-14T21:30:51.935933","exception":false,"start_time":"2022-02-14T21:30:51.837459","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"((list(data['processedText']))[0])","metadata":{"papermill":{"duration":0.10523,"end_time":"2022-02-14T21:30:52.131618","exception":false,"start_time":"2022-02-14T21:30:52.026388","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = ''\ntrainingDocs = list(data['processedText'])\nfor doc in trainingDocs:\n    texts = texts + doc\n","metadata":{"papermill":{"duration":0.195325,"end_time":"2022-02-14T21:30:52.418133","exception":false,"start_time":"2022-02-14T21:30:52.222808","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(texts.split())","metadata":{"papermill":{"duration":2.132653,"end_time":"2022-02-14T21:30:54.643607","exception":false,"start_time":"2022-02-14T21:30:52.510954","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TorchVocab(object):\n    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n    Attributes:\n        freqs: A collections.Counter object holding the frequencies of tokens\n            in the data used to build the Vocab.\n        stoi: A collections.defaultdict instance mapping token strings to\n            numerical identifiers.\n        itos: A list of token strings indexed by their numerical identifiers.\n    \"\"\"\n\n    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n                 vectors=None, unk_init=None, vectors_cache=None):\n        \"\"\"Create a Vocab object from a collections.Counter.\n        Arguments:\n            counter: collections.Counter object holding the frequencies of\n                each value found in the data.\n            max_size: The maximum size of the vocabulary, or None for no\n                maximum. Default: None.\n            min_freq: The minimum frequency needed to include a token in the\n                vocabulary. Values less than 1 will be set to 1. Default: 1.\n            specials: The list of special tokens (e.g., padding or eos) that\n                will be prepended to the vocabulary in addition to an <unk>\n                token. Default: ['<pad>']\n            vectors: One of either the available pretrained vectors\n                or custom pretrained vectors (see Vocab.load_vectors);\n                or a list of aforementioned vectors\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n        \"\"\"\n        self.freqs = counter\n        counter = counter.copy()\n        min_freq = max(min_freq, 1)\n\n        self.itos = list(specials)\n        # frequencies of special tokens are not counted when building vocabulary\n        # in frequency order\n        for tok in specials:\n            del counter[tok]\n\n        max_size = None if max_size is None else max_size + len(self.itos)\n\n        # sort by frequency, then alphabetically\n        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n\n        for word, freq in words_and_frequencies:\n            if freq < min_freq or len(self.itos) == max_size:\n                break\n            self.itos.append(word)\n\n        # stoi is simply a reverse dict for itos\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n        self.vectors = None\n        if vectors is not None:\n            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n        else:\n            assert unk_init is None and vectors_cache is None\n\n    def __eq__(self, other):\n        if self.freqs != other.freqs:\n            return False\n        if self.stoi != other.stoi:\n            return False\n        if self.itos != other.itos:\n            return False\n        if self.vectors != other.vectors:\n            return False\n        return True\n\n    def __len__(self):\n        return len(self.itos)\n\n    def vocab_rerank(self):\n        self.stoi = {word: i for i, word in enumerate(self.itos)}\n\n    def extend(self, v, sort=False):\n        words = sorted(v.itos) if sort else v.itos\n        for w in words:\n            if w not in self.stoi:\n                self.itos.append(w)\n                self.stoi[w] = len(self.itos) - 1\n\n\nclass Vocab(TorchVocab):\n    def __init__(self, counter, max_size=None, min_freq=1):\n        self.pad_index = 0\n        self.unk_index = 1\n        self.eos_index = 2\n        self.sos_index = 3\n        self.mask_index = 4\n        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n                         max_size=max_size, min_freq=min_freq)\n\n    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n        pass\n\n    def from_seq(self, seq, join=False, with_pad=False):\n        pass\n\n    @staticmethod\n    def load_vocab(vocab_path: str) -> 'Vocab':\n        with open(vocab_path, \"rb\") as f:\n            return pickle.load(f)\n\n    def save_vocab(self, vocab_path):\n        with open(vocab_path, \"wb\") as f:\n            pickle.dump(self, f)\n\n\n# Building Vocab with text files\nclass WordVocab(Vocab):\n    def __init__(self, texts, max_size=None, min_freq=1):\n        print(\"Building Vocab\")\n        counter = Counter()\n        count = 0\n        words = ((texts.replace(\"\\n\", \"\").replace(\"\\t\", \"\")).replace(\"|\",\" \")).split()\n        for word in words:\n            counter[word] += 1\n            count += 1\n        print(\"\\nTotal line\", count)\n        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n\n    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n        if isinstance(sentence, str):\n            sentence = sentence.split()\n\n        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n\n        if with_eos:\n            seq += [self.eos_index]  # this would be index 1\n        if with_sos:\n            seq = [self.sos_index] + seq\n\n        origin_seq_len = len(seq)\n\n        if seq_len is None:\n            pass\n        elif len(seq) <= seq_len:\n            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n        else:\n            seq = seq[:seq_len]\n\n        return (seq, origin_seq_len) if with_len else seq\n\n    def from_seq(self, seq, join=False, with_pad=False):\n        words = [self.itos[idx]\n                 if idx < len(self.itos)\n                 else \"<%d>\" % idx\n                 for idx in seq\n                 if not with_pad or idx != self.pad_index]\n\n        return \" \".join(words) if join else words\n\n    @staticmethod\n    def load_vocab(vocab_path: str) -> 'WordVocab':\n        with open(vocab_path, \"rb\") as f:\n            return pickle.load(f)","metadata":{"papermill":{"duration":0.13129,"end_time":"2022-02-14T21:30:54.868056","exception":false,"start_time":"2022-02-14T21:30:54.736766","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab = WordVocab(texts,100000,1)","metadata":{"papermill":{"duration":12.491263,"end_time":"2022-02-14T21:31:07.450228","exception":false,"start_time":"2022-02-14T21:30:54.958965","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(newvocab.stoi)","metadata":{"papermill":{"duration":0.11644,"end_time":"2022-02-14T21:31:07.677446","exception":false,"start_time":"2022-02-14T21:31:07.561006","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('vocabOn30k2minFreqNew.txt' , 'w') as f:\n    for key, val in (newvocab.stoi).items():\n        f.write(\"%s\\t%s\\n\"%(key,val))","metadata":{"papermill":{"duration":0.153023,"end_time":"2022-02-14T21:31:07.939089","exception":false,"start_time":"2022-02-14T21:31:07.786066","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab.save_vocab('vocabOn30k2minfreqNew.txt')","metadata":{"papermill":{"duration":0.14696,"end_time":"2022-02-14T21:31:08.196228","exception":false,"start_time":"2022-02-14T21:31:08.049268","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = WordVocab.load_vocab('vocabOn30k2minfreqNew.txt')\nlen(newvocab.stoi)","metadata":{"papermill":{"duration":0.142448,"end_time":"2022-02-14T21:31:08.448357","exception":false,"start_time":"2022-02-14T21:31:08.305909","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with open('vocabOn30k2minfreqNew.txt', \"rb\") as f:\n    #print(len(f))","metadata":{"papermill":{"duration":0.114995,"end_time":"2022-02-14T21:31:08.674148","exception":false,"start_time":"2022-02-14T21:31:08.559153","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#newvocab.counter","metadata":{"papermill":{"duration":0.117748,"end_time":"2022-02-14T21:31:08.89832","exception":false,"start_time":"2022-02-14T21:31:08.780572","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['processedText'][0]","metadata":{"papermill":{"duration":0.120831,"end_time":"2022-02-14T21:31:09.128563","exception":false,"start_time":"2022-02-14T21:31:09.007732","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch ","metadata":{"papermill":{"duration":12.019566,"end_time":"2022-02-14T21:31:21.259507","exception":false,"start_time":"2022-02-14T21:31:09.239941","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math","metadata":{"papermill":{"duration":0.119012,"end_time":"2022-02-14T21:31:21.491115","exception":false,"start_time":"2022-02-14T21:31:21.372103","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom gensim.models.keyedvectors import KeyedVectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch","metadata":{"papermill":{"duration":0.123499,"end_time":"2022-02-14T21:31:21.72488","exception":false,"start_time":"2022-02-14T21:31:21.601381","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentEmbedding(nn.Embedding):\n    def __init__(self, embed_size=512):\n        super().__init__(3, embed_size, padding_idx=0)","metadata":{"papermill":{"duration":0.120945,"end_time":"2022-02-14T21:31:21.955963","exception":false,"start_time":"2022-02-14T21:31:21.835018","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]","metadata":{"papermill":{"duration":0.125421,"end_time":"2022-02-14T21:31:22.192963","exception":false,"start_time":"2022-02-14T21:31:22.067542","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#newvocab.stoi.keys()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_model = KeyedVectors.load_word2vec_format(\"/kaggle/input/pmcmodel/PMC-w2v.bin\", binary=True)\nprint('Found %s word vectors.' % len(word_model.index_to_key))\nprint(word_model['Cholera'].shape)\nmatrix_len = len(newvocab.stoi)\nweights_matrix = np.zeros((matrix_len, 200))\nwords_found = 0\n\nfor word, i in (newvocab.stoi.items()):\n    try: \n        weights_matrix[i] = word_model[word]\n        words_found += 1\n    except KeyError:\n        weights_matrix[i] = np.random.normal(scale=0.6, size=(200, ))\nprint('words not found',len(newvocab.stoi)- words_found, weights_matrix.shape )\nweights_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_emb_layer(src_vocab_size,embed_size,non_trainable=False):\n    #num_embeddings, embedding_dim = weights_matrix.shape()\n    emb_layer = nn.Embedding(src_vocab_size, embed_size)\n    #emb_layer.load_state_dict({'weight': weights_matrix})\n    emb_layer.weight=nn.Parameter(torch.tensor(weights_matrix,dtype=torch.float32))\n    if non_trainable:\n        emb_layer.weight.requires_grad = False\n\n    return emb_layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenEmbedding(nn.Embedding):\n    def __init__(self, vocab_size, embed_size=512):\n        super().__init__(vocab_size, embed_size, padding_idx=0)","metadata":{"papermill":{"duration":0.120644,"end_time":"2022-02-14T21:31:22.428456","exception":false,"start_time":"2022-02-14T21:31:22.307812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTEmbedding(nn.Module):\n    \"\"\"\n    BERT Embedding which is consisted with under features\n        1. TokenEmbedding : normal embedding matrix\n        2. PositionalEmbedding : adding positional information using sin, cos\n        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n\n        sum of all these features are output of BERTEmbedding\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_size, dropout=0.1):\n        \"\"\"\n        :param vocab_size: total vocab size\n        :param embed_size: embedding size of token embedding\n        :param dropout: dropout rate\n        \"\"\"\n        super().__init__()\n        #self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n        self.token = create_emb_layer(vocab_size,embed_size )\n        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n        self.dropout = nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence, segment_label):\n        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n        return self.dropout(x)","metadata":{"papermill":{"duration":0.122822,"end_time":"2022-02-14T21:31:22.663239","exception":false,"start_time":"2022-02-14T21:31:22.540417","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n        #print(\"here is the LayerNorm shape\", self.a_2.shape, self.b_2.shape)\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        #print(\"here is the shape\", x.shape, (self.a_2 * (x - mean) / (std + self.eps)).shape, self.b_2.shape)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"papermill":{"duration":0.121558,"end_time":"2022-02-14T21:31:22.895607","exception":false,"start_time":"2022-02-14T21:31:22.774049","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = GELU()\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n","metadata":{"papermill":{"duration":0.121346,"end_time":"2022-02-14T21:31:23.130111","exception":false,"start_time":"2022-02-14T21:31:23.008765","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GELU(nn.Module):\n    \"\"\"\n    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n    \"\"\"\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))","metadata":{"papermill":{"duration":0.119233,"end_time":"2022-02-14T21:31:23.363635","exception":false,"start_time":"2022-02-14T21:31:23.244402","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n","metadata":{"papermill":{"duration":0.120418,"end_time":"2022-02-14T21:31:23.594558","exception":false,"start_time":"2022-02-14T21:31:23.47414","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"\n    Compute 'Scaled Dot Product Attention\n    \"\"\"\n\n    def forward(self, query, key, value, mask=None, dropout=None):\n        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n                 / math.sqrt(query.size(-1))\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        p_attn = F.softmax(scores, dim=-1)\n\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n\n        return torch.matmul(p_attn, value), p_attn","metadata":{"papermill":{"duration":0.12053,"end_time":"2022-02-14T21:31:23.824406","exception":false,"start_time":"2022-02-14T21:31:23.703876","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    \"\"\"\n    Take in model size and number of heads.\n    \"\"\"\n\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n\n        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n        self.output_linear = nn.Linear(d_model, d_model)\n        self.attention = Attention()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n                             for l, x in zip(self.linear_layers, (query, key, value))]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n\n        return self.output_linear(x)\n","metadata":{"papermill":{"duration":0.126894,"end_time":"2022-02-14T21:31:24.062423","exception":false,"start_time":"2022-02-14T21:31:23.935529","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    Bidirectional Encoder = Transformer (self-attention)\n    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n    \"\"\"\n\n    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n        \"\"\"\n        :param hidden: hidden size of transformer\n        :param attn_heads: head sizes of multi-head attention\n        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, mask):\n        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n        x = self.output_sublayer(x, self.feed_forward)\n        return self.dropout(x)","metadata":{"papermill":{"duration":0.125975,"end_time":"2022-02-14T21:31:24.301056","exception":false,"start_time":"2022-02-14T21:31:24.175081","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n    \"\"\"\n    BERT model : Bidirectional Encoder Representations from Transformers.\n    \"\"\"\n\n    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n        \"\"\"\n        :param vocab_size: vocab_size of total words\n        :param hidden: BERT model hidden size\n        :param n_layers: numbers of Transformer blocks(layers)\n        :param attn_heads: number of attention heads\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.hidden = hidden\n        self.n_layers = n_layers\n        self.attn_heads = attn_heads\n\n        # paper noted they used 4*hidden_size for ff_network_hidden_size\n        self.feed_forward_hidden = hidden * 4\n\n        # embedding for BERT, sum of positional, segment, token embeddings\n        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n\n        # multi-layers transformer blocks, deep network\n        self.transformer_blocks = nn.ModuleList(\n            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, segment_info):\n        # attention masking for padded token\n        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        x = self.embedding(x, segment_info)\n\n        # running over multiple transformer blocks\n        for transformer in self.transformer_blocks:\n            x = transformer.forward(x, mask)\n\n        return x","metadata":{"papermill":{"duration":0.125867,"end_time":"2022-02-14T21:31:24.53757","exception":false,"start_time":"2022-02-14T21:31:24.411703","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert = BERT(len(vocab), hidden=200, n_layers=8, attn_heads=5)","metadata":{"papermill":{"duration":0.675953,"end_time":"2022-02-14T21:31:25.323925","exception":false,"start_time":"2022-02-14T21:31:24.647972","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTLM(nn.Module):\n    \"\"\"\n    BERT Language Model\n    Next Sentence Prediction Model + Masked Language Model\n    \"\"\"\n\n    def __init__(self, bert: BERT, vocab_size):\n        \"\"\"\n        :param bert: BERT model which should be trained\n        :param vocab_size: total vocab size for masked_lm\n        \"\"\"\n\n        super().__init__()\n        self.bert = bert\n        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n\n    def forward(self, x, segment_label):\n        x = self.bert(x, segment_label)\n        return x\n        #return self.next_sentence(x)#self.mask_lm(x)#, \n\n\nclass NextSentencePrediction(nn.Module):\n    \"\"\"\n    2-class classification model : is_next, is_not_next\n    \"\"\"\n\n    def __init__(self, hidden):\n        \"\"\"\n        :param hidden: BERT model output size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, 2)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x[:, 0]))\n\n\nclass MaskedLanguageModel(nn.Module):\n    \"\"\"\n    predicting origin token from masked input sequence\n    n-class classification problem, n-class = vocab_size\n    \"\"\"\n\n    def __init__(self, hidden, vocab_size):\n        \"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x))","metadata":{"papermill":{"duration":0.125802,"end_time":"2022-02-14T21:31:25.561612","exception":false,"start_time":"2022-02-14T21:31:25.43581","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda_condition = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")","metadata":{"papermill":{"duration":0.177764,"end_time":"2022-02-14T21:31:25.85002","exception":false,"start_time":"2022-02-14T21:31:25.672256","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preTrainedModel = BERTLM(bert, len(vocab)).to(device)","metadata":{"papermill":{"duration":5.878524,"end_time":"2022-02-14T21:31:31.844112","exception":false,"start_time":"2022-02-14T21:31:25.965588","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preTrainedModel.load_state_dict(torch.load('/kaggle/input/bert-pretrain-bothtask/45_model_weights.pth'))\npreTrainedModel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in preTrainedModel.parameters():\n    param.requires_grad = False","metadata":{"papermill":{"duration":0.116949,"end_time":"2022-02-14T21:31:32.073224","exception":false,"start_time":"2022-02-14T21:31:31.956275","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiLSTM(nn.Module):\n    def __init__(self, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(hidden, hidden//2, batch_first=True, bidirectional=True)\n    def forward(self,sentenceEncode):\n        out,(h_0,c_0) = self.lstm(sentenceEncode)\n        return h_0.reshape(1,-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test = BiLSTM(hidden = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test(torch.rand(size = (1,4, 200)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #hidden_stae = test(torch.rand(size = (1,6, 200)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hidden_stae.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c.shape, d.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT_FineTuned(nn.Module):\n    \"\"\"\n    BERT model : Bidirectional Encoder Representations from Transformers.\n    \"\"\"\n\n    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n        \"\"\"\n        :param vocab_size: vocab_size of total words\n        :param hidden: BERT model hidden size\n        :param n_layers: numbers of Transformer blocks(layers)\n        :param attn_heads: number of attention heads\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.hidden = hidden\n        self.n_layers = n_layers\n        self.attn_heads = attn_heads\n        self.preTrainedModel = preTrainedModel\n        # paper noted they used 4*hidden_size for ff_network_hidden_size\n        self.feed_forward_hidden = hidden * 4\n\n        # embedding for BERT, sum of positional, segment, token embeddings\n        #self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n\n        # multi-layers transformer blocks, deep network\n        self.transformer_blocks = nn.ModuleList(\n            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, segment_info):\n        # attention masking for padded token\n        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        #x = self.embedding(x, segment_info)\n        x = self.preTrainedModel(x, segment_info)\n        #print(\"\\noutput obtained from pre trained model  \", x.shape, \"\\n\", x)\n        # running over multiple transformer blocks\n        for transformer in self.transformer_blocks:\n            x = transformer.forward(x, mask)\n\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a = torch.rand(size= (4,1,200))\n#b = torch.rand(size= (50,1,200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a.reshape(1,4,200).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a.shape, b.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#batchSize = a.shape[0]\n#embedSize = a.shape[-1]\n#codeCount = b.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b.reshape(embedSize,-1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score = torch.matmul(a,b.reshape(embedSize,-1))\n#code_attn_wt = F.softmax(score, dim=-1)\n#code_attn_wt = code_attn_wt.reshape(-1, codeCount)\n#code_attn =torch.matmul(code_attn_wt, b.reshape(-1,embedSize))\n#code_attn_doc = torch.cat((code_attn.reshape(-1,embedSize), a.reshape(-1,embedSize)), dim = -1)#cancatenating combined code and docuemnt rep","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c = torch.rand(1,200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k = torch.matmul(b,c.reshape(200,-1)).reshape(1,50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b.reshape(-1,200).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F.softmax(k, dim=-1).reshape(50, -1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.sum(torch.tensor([[1,2,3],\n                       #[4,5,6]]), axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r = torch.sum(((b.reshape(-1,200)) * (F.softmax(k, dim=-1).reshape(50, -1))), axis = 0).reshape(-1, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r.shape, c.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#l = torch.cat((c, r), dim = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.cat((l,  b[0]),axis = -1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F.softmax(k, dim=-1).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionModuleForLSTM(nn.Module):\n    \"\"\"\n    Building attention of clinical texts with ICD Codes#100\n    First Calculate the importance of clinical texts with code\n    Generate the common representation of all codes with using normalized weight for a particular clinical doc\n    cancatenate the clinical texts rep, attention rep and code rep\n    Pass the cancate rep through linear layer with sigmoid activation\n    \"\"\"\n    def __init__(self, hidden):\n        super().__init__()\n        self.linear = nn.Linear(3 * hidden, 1)\n        self.sigmoid  = nn.Sigmoid()\n    def forward(self,docEmbed, codeEmbed):\n        #batchSize = docEmbed.shape[0]\n        #docEmbed\n        embedSize = docEmbed.shape[-1] #docEmbed = [1,embedSize], codeEmbed = [codecount, 1,embedSize ]\n        codeCount = codeEmbed.shape[0]\n        score = torch.matmul(codeEmbed,docEmbed.reshape(embedSize,-1)).reshape(1,codeCount)\n        #score = torch.matmul(docEmbed,codeEmbed.reshape(embedSize,-1))\n        code_attn_wt = F.softmax(score, dim=-1).reshape(codeCount, -1)\n        #code_attn_wt = code_attn_wt.reshape(-1, codeCount)\n        #code_attn =torch.matmul(code_attn_wt, codeEmbed.reshape(-1,embedSize))\n        code_attn = torch.sum(((codeEmbed.reshape(-1,embedSize)) * code_attn_wt), axis = 0).reshape(-1, embedSize) # [1, embedSize]\n        code_attn_doc = torch.cat((docEmbed, code_attn), dim = -1)#cancatenating combined code and docuemnt rep\n        output_list = []\n        for index in range(codeCount):\n            code_attn_doc_code = torch.cat((code_attn_doc,  codeEmbed[index]),axis = -1)\n            pred_out = self.sigmoid(self.linear(code_attn_doc_code))\n            #print(pred_out.shape)\n            output_list.append(pred_out)\n        return output_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test1 = AttentionModuleForLSTM(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.tensor(test1(torch.rand(size = (1,200)), torch.rand(size = (50, 1, 200))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hidden_stae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test(torch.rand(size = (1,4, 200)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a = torch.rand(size = (4,1,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a.shape#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a.reshape(1,-1,3).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTLM_FineTuned(nn.Module):\n    \"\"\"\n    BERT Language Model\n    Next Sentence Prediction Model + Masked Language Model\n    \"\"\"\n    \n    def __init__(self, vocab_size):\n        \"\"\"\n        :param bert: BERT model which should be trained\n        :param vocab_size: total vocab size for masked_lm\n        \"\"\"\n\n        super().__init__()\n        self.bert_fineTuned = BERT_FineTuned(len(vocab), hidden=200, n_layers=3, attn_heads=5)\n        self.next_sentence = NextSentencePrediction(self.bert_fineTuned.hidden)\n        self.mask_lm = MaskedLanguageModel(self.bert_fineTuned.hidden, vocab_size)\n        self.attentionModule = AttentionModuleForLSTM(self.bert_fineTuned.hidden)\n        self.bilstm = BiLSTM(self.bert_fineTuned.hidden)\n        #self.attentionModule = AttentionModule(self.bert_fineTuned.hidden)\n        #self.pretrainedModel = preTrainedModel\n\n    def forward(self, x, segment_label, label_input, label_segment):\n        x = self.bert_fineTuned(x, segment_label)\n        sentenceRep = self.bilstm(x.reshape(1,-1,self.bert_fineTuned.hidden))\n        label_input = self.bert_fineTuned(label_input, label_segment)\n        #print(\"\\nembedding has been generated  \", x.shape, label_input.shape, x, label_input)\n        #code_pred = self.attentionModule(x[:,0:1,:],label_input[:,0:1,:])\n        code_pred = self.attentionModule(sentenceRep,label_input[:,0:1,:])\n        return self.mask_lm(x), self.mask_lm(label_input), x, label_input, code_pred\n        #return self.next_sentence(x) self.mask_lm(x), self.next_sentence(label_input), self.mask_lm(label_input)\n\n\nclass NextSentencePrediction(nn.Module):\n    \"\"\"\n    2-class classification model : is_next, is_not_next\n    \"\"\"\n\n    def __init__(self, hidden):\n        \"\"\"\n        :param hidden: BERT model output size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, 2)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x[:, 0]))\n\n\nclass MaskedLanguageModel(nn.Module):\n    \"\"\"\n    predicting origin token from masked input sequence\n    n-class classification problem, n-class = vocab_size\n    \"\"\"\n\n    def __init__(self, hidden, vocab_size):\n        \"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(hidden, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n        \n    def forward(self, x):\n        return self.softmax(self.linear(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTLM_FineTuned(len(vocab)).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport tqdm\nimport torch\nimport random","metadata":{"papermill":{"duration":0.119804,"end_time":"2022-02-14T21:31:32.302712","exception":false,"start_time":"2022-02-14T21:31:32.182908","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Preparing Dataset and Dataloadrer ","metadata":{"papermill":{"duration":0.11741,"end_time":"2022-02-14T21:31:32.531734","exception":false,"start_time":"2022-02-14T21:31:32.414324","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icdCodeDescription  = '/kaggle/input/mimicdata/ICD_desc_with_freq.csv'\ntopCodes  = 50\nicdCodes = pd.read_csv(icdCodeDescription)[:topCodes]\nicdCodeDescDict = {}\ncodes  = list(icdCodes[\"Code\"])\nDesc  = list(icdCodes[\"Long Description\"])\ncode2Index = {}\nfor i , code in enumerate(codes):\n    icdCodeDescDict[code] = Desc[i]\n    code2Index[code] = i+1\n#return icdCodeDescDict,code2Index","metadata":{"papermill":{"duration":0.153625,"end_time":"2022-02-14T21:31:32.796076","exception":false,"start_time":"2022-02-14T21:31:32.642451","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(icdCodeDescDict)","metadata":{"papermill":{"duration":0.119164,"end_time":"2022-02-14T21:31:33.025946","exception":false,"start_time":"2022-02-14T21:31:32.906782","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"papermill":{"duration":0.123066,"end_time":"2022-02-14T21:31:33.261039","exception":false,"start_time":"2022-02-14T21:31:33.137973","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CreateTextFileLableMap():\n    trueLabels = list(data['ICD9_CODE'])\n    textFileLabel = {}\n    for i , trueLabel in enumerate(trueLabels):\n        textFileLabel[i+1]  = [y for y in trueLabel[2:-2].split(\"', '\")if y in  icdCodeDescDict]\n    return textFileLabel","metadata":{"papermill":{"duration":0.120329,"end_time":"2022-02-14T21:31:33.492884","exception":false,"start_time":"2022-02-14T21:31:33.372555","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = CreateTextFileLableMap() # file index starts form 1","metadata":{"papermill":{"duration":0.179257,"end_time":"2022-02-14T21:31:33.782631","exception":false,"start_time":"2022-02-14T21:31:33.603374","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label[1]","metadata":{"papermill":{"duration":0.122524,"end_time":"2022-02-14T21:31:34.015386","exception":false,"start_time":"2022-02-14T21:31:33.892862","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['ICD9_CODE'][0],data['top_ICD'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listIds = [i for i in range(data.shape[0])]","metadata":{"papermill":{"duration":0.122129,"end_time":"2022-02-14T21:31:34.249796","exception":false,"start_time":"2022-02-14T21:31:34.127667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(listIds)","metadata":{"papermill":{"duration":0.118756,"end_time":"2022-02-14T21:31:34.479913","exception":false,"start_time":"2022-02-14T21:31:34.361157","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n  'Characterizes a dataset for PyTorch'\n  def __init__(self, list_IDs, labels):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n        ID = self.list_IDs[index]\n\n        # Load data and get label\n        X = data['processedText'][ID]\n        y = self.labels[ID+1]\n        return X, y","metadata":{"papermill":{"duration":0.12007,"end_time":"2022-02-14T21:31:34.709476","exception":false,"start_time":"2022-02-14T21:31:34.589406","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset(listIds,label )","metadata":{"papermill":{"duration":0.122034,"end_time":"2022-02-14T21:31:34.940489","exception":false,"start_time":"2022-02-14T21:31:34.818455","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0][1]","metadata":{"papermill":{"duration":0.121966,"end_time":"2022-02-14T21:31:35.174047","exception":false,"start_time":"2022-02-14T21:31:35.052081","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset[0][0].split(\"|\")[4].split())","metadata":{"papermill":{"duration":0.120831,"end_time":"2022-02-14T21:31:35.408751","exception":false,"start_time":"2022-02-14T21:31:35.28792","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = (data['processedText'][0].split('|'))","metadata":{"papermill":{"duration":0.111397,"end_time":"2022-02-14T21:31:35.631942","exception":false,"start_time":"2022-02-14T21:31:35.520545","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newvocab.stoi.get('medications', newvocab.unk_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[newvocab.stoi.get(word, newvocab.unk_index) for word in 'were restarted on regimen at discharge the blood in your vomit was to'.split()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self,vocab, seq_len, list_IDs, labels, encoding=\"utf-8\", on_memory=True):\n        self.vocab = vocab\n        self.seq_len = seq_len\n        self.codeDesc  = icdCodeDescDict\n        self.on_memory = on_memory\n        #self.corpus_lines = corpus_lines\n        self.list_IDs  = list_IDs \n        self.encoding = encoding\n        self.labels = labels\n        self.texts  = data['processedText']\n        self.desc = Desc\n        self.CodeIndex = code2Index\n\n\n    def __len__(self):\n        return len(self.list_IDs)\n    def codelabel(self, trueLabels):\n        labelIndexes = [int(self.CodeIndex[trueLabel])-1 for trueLabel in trueLabels]\n        x = torch.LongTensor(labelIndexes)\n        y_onehot = nn.functional.one_hot(x, num_classes=50)#change label count \n        y_onehot = y_onehot.sum(dim=0).float()\n        #y_onehot = y_onehot.reshape(-1,1)\n        return y_onehot\n    \n    def __getitem__(self, item):# if item count start from 1 and till the size of docuemts length \n        trueLabels = self.labels[item]\n        text_ = self.texts[item-1]\n        text_ = text_.split('|')\n        if(len(text_) % 2): #To make even number of sentences\n            text_.append(text_[-1])\n        text_ = [text_[i: i+2] for i in range(0, len(text_), 2)]\n        #print(len(text_))\n        label_input = self.codelabel(trueLabels)\n        bert_input_ = self.generateTensor(text_)\n        return bert_input_, label_input\n        #return {key: torch.tensor(value) for key, value in output.items()}\n    def generateTensorOnLabels(self):\n        codeDesc  = [text for text in self.desc]\n        if(len(codeDesc) % 2): #To make even number of sentences\n            codeDesc.append(codeDesc[-1])\n        #codeDesc = [codeDesc[i: i+2] for i in range(0, len(codeDesc), 2)]\n        #print(codeDesc)\n        numberOfLines  = len(codeDesc)\n        bert_input_ = []\n        bert_label_ = []\n        segment_label_ = []\n        is_next_ = []\n        for i in range(numberOfLines):\n            is_next_label = 0\n            t1 = codeDesc[i]\n            #t1, t2, is_next_label = self.random_sent_for_Code_Desc(i, codeDesc)#generate the label for next sentence prediction in the form of 0 or 1\n            #print(\"sentence 2 is\", t2)\n            t1_random, t1_label = self.random_word(t1)#This will generate the masked token for sentence A and the label index for the masked token \n            #t2_random, t2_label = self.random_word(t2)\n\n        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n            t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n            #t2 = t2_random + [self.vocab.eos_index]\n\n            t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n            #t2_label = t2_label + [self.vocab.pad_index]\n\n            #segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n            segment_label = ([1 for _ in range(len(t1))])[:self.seq_len]\n            bert_input = (t1)[:self.seq_len]\n            #bert_input = (t1 + t2)[:self.seq_len]\n            bert_label = (t1_label)[:self.seq_len]\n            #bert_label = (t1_label + t2_label)[:self.seq_len]\n\n            padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]#This would pad the input, output lable and sengement label up to the maximum length\n            bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n            bert_input_.append(bert_input)\n            bert_label_.append(bert_label)\n            segment_label_.append(segment_label)\n            is_next_.append(is_next_label)\n        return ([torch.tensor(bert_input_),torch.tensor(bert_label_),torch.tensor(segment_label_), torch.tensor(is_next_)])\n    def generateTensor(self, text):\n        numberOfLines  = len(text)\n        bert_input_ = []\n        bert_label_ = []\n        segment_label_ = []\n        is_next_ = []\n        for i in range(numberOfLines):\n            t1, t2, is_next_label = self.random_sent(i, text)#generate the label for next sentence prediction in the form of 0 or 1\n            t1_random, t1_label = self.random_word(t1)#This will generate the masked token for sentence A and the label index for the masked token \n            t2_random, t2_label = self.random_word(t2)\n\n        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n            t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n            t2 = t2_random + [self.vocab.eos_index]\n\n            t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n            t2_label = t2_label + [self.vocab.pad_index]\n\n            segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n            bert_input = (t1 + t2)[:self.seq_len]\n            bert_label = (t1_label + t2_label)[:self.seq_len]\n\n            padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]#This would pad the input, output lable and sengement label up to the maximum length\n            bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n            bert_input_.append(bert_input)\n            bert_label_.append(bert_label)\n            segment_label_.append(segment_label)\n            is_next_.append(is_next_label)\n        return ([torch.tensor(bert_input_),torch.tensor(bert_label_),torch.tensor(segment_label_), torch.tensor(is_next_)])\n    def random_word(self, sentence):\n        tokens = sentence.split()\n        output_label = []\n\n        for i, token in enumerate(tokens):\n            prob = random.random()\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    tokens[i] = self.vocab.mask_index\n\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    tokens[i] = random.randrange(len(self.vocab))\n\n                # 10% randomly change token to current token\n                else:\n                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n\n                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n\n            else:\n                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n                output_label.append(0)\n\n        return tokens, output_label\n\n    def random_sent(self, index,text):\n        t1, t2 = self.get_corpus_line(index,text)\n\n        # output_text, label(isNotNext:0, isNext:1)\n        if random.random() > 0.5:\n            return t1, t2, 1\n        else:\n            return t1, t2, 1\n        \n    def random_sent_for_Code_Desc(self, index,text):\n        t1, t2 = self.get_corpus_line(index,text)\n\n        # output_text, label(isNotNext:0, isNext:1)\n        if random.random() > 0.5:\n            return t1, t2, 0\n        else:\n            return t1, t2, 0\n    \n    def get_corpus_line(self, item,text):\n        if self.on_memory:\n            return text[item][0], text[item][1]\n        \"\"\"else:\n            line = self.file.__next__()\n            if line is None:\n                self.file.close()\n                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n                line = self.file.__next__()\n\n            t1, t2 = line[:-1].split(\"\\t\")\n            return t1, t2\"\"\"\n\n    def get_random_line(self,text):\n        if self.on_memory:\n            return text[random.randrange(len(text))][1]\n    def topCode(self):\n        codeInput = self.generateTensorOnLabels()\n        return codeInput","metadata":{"papermill":{"duration":0.146653,"end_time":"2022-02-14T21:31:36.840122","exception":false,"start_time":"2022-02-14T21:31:36.693469","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x = torch.LongTensor([1,2,3,4])\n#y_onehot = nn.functional.one_hot(x, num_classes=50)#change label count \n#y_onehot = y_onehot.sum(dim=0).float()\n#y_onehot = y_onehot.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_onehot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#binayLoss(y_onehot,y_onehot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = BERTDataset(newvocab,256, listIds,label )","metadata":{"papermill":{"duration":0.126844,"end_time":"2022-02-14T21:31:37.085908","exception":false,"start_time":"2022-02-14T21:31:36.959064","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a  = data['processedText'][0]","metadata":{"papermill":{"duration":0.121217,"end_time":"2022-02-14T21:31:37.320718","exception":false,"start_time":"2022-02-14T21:31:37.199501","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data[1][0].shape, len(train_data[1])","metadata":{"papermill":{"duration":0.126937,"end_time":"2022-02-14T21:31:37.562915","exception":false,"start_time":"2022-02-14T21:31:37.435978","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\n","metadata":{"papermill":{"duration":0.120805,"end_time":"2022-02-14T21:31:37.797333","exception":false,"start_time":"2022-02-14T21:31:37.676528","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument(\"-hs\", \"--hidden\", type=int, default=512, help=\"hidden size of transformer model\")\nparser.add_argument(\"-l\", \"--layers\", type=int, default=8, help=\"number of layers\")\nparser.add_argument(\"-a\", \"--attn_heads\", type=int, default=8, help=\"number of attention heads\")\nparser.add_argument(\"-s\", \"--seq_len\", type=int, default=20, help=\"maximum sequence len\")\n\nparser.add_argument(\"-b\", \"--batch_size\", type=int, default=64, help=\"number of batch_size\")\nparser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"number of epochs\")\nparser.add_argument(\"-w\", \"--num_workers\", type=int, default=5, help=\"dataloader worker size\")\nparser.add_argument(\"-steps\", \"--warm_up_step\", type=int, default=1000, help=\"dataloader worker size\")\n\nparser.add_argument(\"--with_cuda\", type=bool, default=True, help=\"training with CUDA: true, or false\")\nparser.add_argument(\"--log_freq\", type=int, default=10, help=\"printing loss every n iter: setting n\")\nparser.add_argument(\"--corpus_lines\", type=int, default=None, help=\"total number of lines in corpus\")\nparser.add_argument(\"--cuda_devices\", type=int, nargs='+', default=None, help=\"CUDA device ids\")\nparser.add_argument(\"--on_memory\", type=bool, default=True, help=\"Loading on memory: true or false\")\n\nparser.add_argument(\"--lr\", type=float, default=1e-3, help=\"learning rate of adam\")\nparser.add_argument(\"--adam_weight_decay\", type=float, default=0.01, help=\"weight_decay of adam\")\nparser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\nparser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam first beta value\")\nargs = parser.parse_args(\"\")","metadata":{"papermill":{"duration":0.248983,"end_time":"2022-02-14T21:31:38.160681","exception":false,"start_time":"2022-02-14T21:31:37.911698","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"betas=(0.9, 0.999)\n","metadata":{"papermill":{"duration":0.211871,"end_time":"2022-02-14T21:31:38.613912","exception":false,"start_time":"2022-02-14T21:31:38.402041","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args","metadata":{"papermill":{"duration":0.125084,"end_time":"2022-02-14T21:31:38.865525","exception":false,"start_time":"2022-02-14T21:31:38.740441","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define model hyper parameter","metadata":{"papermill":{"duration":0.115965,"end_time":"2022-02-14T21:31:39.099401","exception":false,"start_time":"2022-02-14T21:31:38.983436","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.n_warmup_steps = n_warmup_steps\n        self.n_current_steps = 0\n        self.init_lr = np.power(d_model, -0.5)\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n    def zero_grad(self):\n        \"Zero out the gradients by the inner optimizer\"\n        self._optimizer.zero_grad()\n\n    def _get_lr_scale(self):\n        return np.min([\n            np.power(self.n_current_steps, -0.5),\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_current_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr","metadata":{"papermill":{"duration":0.132214,"end_time":"2022-02-14T21:31:39.345993","exception":false,"start_time":"2022-02-14T21:31:39.213779","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import Adam\noptim = Adam(model.parameters(), lr=args.lr, betas=betas, weight_decay=args.adam_weight_decay)\noptim_schedule = ScheduledOptim(optim, args.hidden, n_warmup_steps=args.warm_up_step)\n#criterion = nn.NLLLoss(ignore_index=0)\n#log_freq = args.log_freq","metadata":{"papermill":{"duration":0.123852,"end_time":"2022-02-14T21:31:39.585141","exception":false,"start_time":"2022-02-14T21:31:39.461289","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.parameters()\ncriterion = nn.NLLLoss(ignore_index=0)\nbinayLoss = nn.BCELoss()","metadata":{"papermill":{"duration":0.121495,"end_time":"2022-02-14T21:31:39.822691","exception":false,"start_time":"2022-02-14T21:31:39.701196","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_loss = 0.0\ntotal_correct = 0\ntotal_element = 0","metadata":{"papermill":{"duration":0.136934,"end_time":"2022-02-14T21:31:40.075623","exception":false,"start_time":"2022-02-14T21:31:39.938689","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampleData, b = train_data[39]","metadata":{"papermill":{"duration":0.124996,"end_time":"2022-02-14T21:31:40.315184","exception":false,"start_time":"2022-02-14T21:31:40.190188","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampleData[0].shape","metadata":{"papermill":{"duration":0.125234,"end_time":"2022-02-14T21:31:40.55662","exception":false,"start_time":"2022-02-14T21:31:40.431386","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docIdxs = [i+1 for i in range(len(train_data))] #starts from index 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.train()","metadata":{"papermill":{"duration":0.124002,"end_time":"2022-02-14T21:31:40.796586","exception":false,"start_time":"2022-02-14T21:31:40.672584","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#(sampleData[2][3:4,:])","metadata":{"papermill":{"duration":0.13152,"end_time":"2022-02-14T21:31:41.044936","exception":false,"start_time":"2022-02-14T21:31:40.913416","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#next_sent_output, mask_lm_output = model.forward(sampleData[0].to(device), sampleData[2].to(device))","metadata":{"papermill":{"duration":0.123935,"end_time":"2022-02-14T21:31:41.285348","exception":false,"start_time":"2022-02-14T21:31:41.161413","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#next_sent_output.shape, mask_lm_output.shape","metadata":{"papermill":{"duration":0.123312,"end_time":"2022-02-14T21:31:41.526087","exception":false,"start_time":"2022-02-14T21:31:41.402775","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"papermill":{"duration":0.124542,"end_time":"2022-02-14T21:31:41.766043","exception":false,"start_time":"2022-02-14T21:31:41.641501","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_out(predList):\n    return (torch.vstack(code_pred).reshape(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_trueLabel(labelTensors_,batchSize):\n    return (labelTensors_.repeat(1,batchSize)).reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calBinaryLoss(batchSize, codePred,label_tensors):\n    pred_tens = process_out(codePred)\n    true_tens = process_trueLabel(label_tensors, batchSize)\n    #print(\"\\nhere is the label shape \",pred_tens.shape,  true_tens.shape)\n    return binayLoss(pred_tens, true_tens.to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nloss_val = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 1\nfor epoch in range(epochs):\n    print('Training for epoch ',epoch+1)\n    random.shuffle(docIdxs)\n    codeTensor = train_data.topCode()\n    start = time.time()\n    for docIdx in docIdxs:\n        #print(docIdx)\n        trainTensors, labelTensors = train_data[docIdx]\n        batch_Size = trainTensors[0].shape[0]\n        #print(trainTensors[0].shape)\n        mask_lm_output, code_mask_lm_out, bert_text, bert_codeText, code_pred = model(trainTensors[0].to(device), trainTensors[2].to(device), codeTensor[0].to(device), codeTensor[2].to(device))\n        #next_loss = criterion(next_sent_output, trainTensors[3].to(device))\n        \n        #print(\"\\t\",batch_Size )\n        #print(bert_text.shape,bert_codeText.shape, mask_lm_output.shape, code_mask_lm_out.shape)\n        mask_loss_text = criterion(mask_lm_output.transpose(1, 2), trainTensors[1].to(device))\n        mask_loss_code = criterion(code_mask_lm_out.transpose(1, 2), codeTensor[1].to(device))\n        attentionLoss = binayLoss(torch.tensor(code_pred).to(device),labelTensors.to(device) )\n        loss = mask_loss_text + mask_loss_code + attentionLoss #calBinaryLoss(batch_Size, code_pred,labelTensors ) #+ next_loss\n        if True:\n                optim_schedule.zero_grad()\n                loss.backward()\n                optim_schedule.step_and_update_lr()\n        # next sentence prediction accuracy\n        #correct = next_sent_output.argmax(dim=-1).eq(trainTensors[3].to(device)).sum().item()\n        #avg_loss += loss.item()\n        avg_loss += (loss.item())/trainTensors[0].shape[0]\n        #total_correct += correct\n        total_element += trainTensors[3].nelement()\n    #print(\"avg_acc\",(total_correct / (total_element ))* 100)\n    print(\"avg_loss\", avg_loss / (epoch + 1))\n    loss_val.append((avg_loss / (epoch + 1) )/len(train_data))\n    if((epoch+3) % 3==0):\n        torch.save(model.state_dict(), str(epoch)+'_model_weights.pth')\n        torch.save(model, str(epoch)+'_bertML.model')\n    print(\"loss val\", loss_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print((time.time()-start)/60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, str(epochs)+'_bertML.model')","metadata":{"papermill":{"duration":0.801673,"end_time":"2022-02-15T02:12:38.373035","exception":false,"start_time":"2022-02-15T02:12:37.571362","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), str(epochs)+'_model_weights.pth')","metadata":{"papermill":{"duration":0.768934,"end_time":"2022-02-15T02:12:39.263435","exception":false,"start_time":"2022-02-15T02:12:38.494501","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for model loss\nimport matplotlib.pyplot as plt\nplt.plot(loss_val)\n#plt.plot(train_history1.history['val_loss'])\nplt.title('Model Loss')\n#plt.ylabel('validation loss')\nplt.xlabel('epoch')\nplt.legend(['train loss'], loc='upper left')\nplt.show()\n#plt.savefig('Loss_trainVsValidation.png', bbox_inches='tight', dpi = 300)","metadata":{"papermill":{"duration":0.121497,"end_time":"2022-02-15T02:12:39.508155","exception":false,"start_time":"2022-02-15T02:12:39.386658","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-02T14:48:15.456449Z","iopub.status.idle":"2022-05-02T14:48:15.457078Z","shell.execute_reply.started":"2022-05-02T14:48:15.456841Z","shell.execute_reply":"2022-05-02T14:48:15.456865Z"},"trusted":true},"execution_count":null,"outputs":[]}]}